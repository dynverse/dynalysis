  \myhypothesis{ti_evaluation}{ti\_evaluation}{102,102,102}{Dynamic processes can be modelled and studied with TI methods}{* Characterise and organise TI methodology @characterisation\\
    * Thoroughly set up experiment design to ensure correct scientific reasoning @experiment\_design\\
    * Train TI methods on synthetic data @synthetic\_testing\\
    * Evaluate them on biological data @biological\_validation\\
    * Provide reader with a set of instructions on which TI method to use on which kinds of datasets @guidelines}{Some TI methods will perform well on some datasets.\\
    Performance will depend on parameters.}{An extensive TI evaluation will:\\
    * Stimulate the adoption of TI methods\\
    * Stimulate the development of more robust and accurate TI methods\\
    * Stimulate the evolution of the TI to tackle more complex tasks}

  \myhypothesis{characterisation}{characterisation}{52,152,219}{We characterised and organised TI methodology in terms of which biological question is being asked, how a TI method tries to solve this question, and what characteristics it has}{We need to characterise \\
    * the tasks / biological questions @characterise\_tasks\\
    * the platforms @characterise\_platforms\\
    * the methods @characterise\_methods}{}{There is a lot of confusion about what TI is, what problems can be solved with it, and what sort of solution you might expect from a TI method. The characterisation will provide a structure to link everything together and clarify ambiguities}

  \myhypothesis{characterise_tasks}{characterise\_tasks}{52,152,219}{We characterised the different TI tasks}{TODO}{}{}

  \myhypothesis{characterise_platforms}{characterise\_platforms}{52,152,219}{We collected properties of possible datasets and determined which TI task could be used on the dataset}{TODO}{}{}

  \myhypothesis{characterise_methods}{characterise\_methods}{52,152,219}{We characterised TI methods in terms of its inputs, outputs, methodology and type of TI task it tries to solve}{TODO}{}{}

  \myhypothesis{experiment_design}{experiment\_design}{231,76,60}{Our evaluation of TI methods is scientifically sound}{Construct and evaluate the:\\
    * synthetic gold standard\\
    * biological gold standard\\
    * benchmark pipeline}{}{Good and scientifically sound TI evaluation methodology is required in order for the field to progress further}

  \myhypothesis{construct_synthetic_gold}{construct\_synthetic\_gold}{231,76,60}{We generated realistic synthetic expression data of cells as part of a dynamic process}{Determine:\\
    * the gene regulation model @regulation\_model\\
    * the transcription @transcription\_model\\
    * the dynamic process model @dynamic\_process\_model\\
    * the sequencing model @platforms\_model}{}{}

  \myhypothesis{regulation_model}{regulation\_model}{231,76,60}{We use GRNs as a model for gene regulation. These GRNs are derived from real regulatory networks}{TODO}{}{}

  \myhypothesis{transcription_model}{transcription\_model}{231,76,60}{We can generate expression given a GRN}{TODO}{}{}

  \myhypothesis{dynamic_process_model}{dynamic\_process\_model}{231,76,60}{We can translate a dynamic process into a GRN such that generated expression therefrom resembles observations of cells following this dynamic process}{TODO}{}{}

  \myhypothesis{platforms_model}{platforms\_model}{231,76,60}{We determined a model of the technical noise generated by observing the state of a cell through RNA sequencing}{Compare real data across platforms with the simulated data across platforms\\
    - Drop-out rates\\
    - Distributions\\
    - Compare with "ideal" platform}{}{Platforms could induce a bias in the performance of certain methods, so it is important that our platform data is comparable to real platform data\\
    This will allow us to investigate the advantages and disadvantages of different platforms}

  \myhypothesis{generate_synthetic_data}{generate\_synthetic\_data}{231,76,60}{We generated synthetic data in accordance to our models: \\
     * @regulation\_model\\
     * @transcription\_model\\
     * @dynamic\_process\_model\\
     * @platforms\_model}{We simulate using SDEs (with enough noise) and Gillespie SSA (with noise and single-molecule) which takes into account\\
    * Technical bias introduced by platforms @simulating\_platform\_noise\\
    * Biological\\
     - Modular\\
     - Dim red and example of single-cell paths}{N/A}{We need to generate realistic (or at least useful) expression data from GRNs in order to be able to prioritise TI methods for real datasets}

  \myhypothesis{timings_synthetic_generator}{timings\_synthetic\_generator}{231,76,60}{Our SSA implementation is /blazing fast/.\\
    Goal: simulate 100'000 cells with 20'000 genes in <24 hours.\\
    10-fold sampling of single cells can be allowed.}{Perform timings experiments on our implementation of fastgssa versus that of GillespieSSA.}{fastgssa is very faster, but still outputs very similar data for the same given network}{If we want to generate realistic data, we need to be able to simulate as many cells and as many genes as would hopefully occur in near-future experiments}

  \myhypothesis{construct_biological_gold}{construct\_biological\_gold}{231,76,60}{We collected high-quality datasets containing cells along a dynamic process, of which we know the state of each of the cells}{* Collect data\\
    * Perform some kind of tests to ensure there is enough signal-to-noise?}{}{}

  \myhypothesis{benchmarking_pipeline}{benchmarking\_pipeline}{231,76,60}{We use a benchmarking pipeline to train the parameters of the methods and ensure that the datasets the methods are evaluated on are unrelated to avoid overfitting}{For each of the TI tasks, we train the parameters of the methods on synthetic datasets and evaluate them on biological datasets}{}{}

  \myhypothesis{metrics}{metrics}{231,76,60}{We have several good metrics to evaluate different aspects of predicting a good trajectory:\\
      * the main structure of the predicted trajectory should be good   \\
      * the ordering of the cells should be good\\
      * metric with distance from origin}{Unit test metrics with toy examples}{Each metric makes sense theoretically and fulfill the requirements of the toy example, although they can focus different aspects (or levels) of the TI model}{Good metrics are the foundation of a good evaluation}

  \myhypothesis{method_fairness}{method\_fairness}{231,76,60}{We treat each TI method fairly}{* No TI method is left out\\
    * Used implementation from original authors if available\\
    * Made own implementation if necessary; contacted original authors to verify implementation\\
    * Described all sorts of parameters, which will be optimised later}{}{}

  \myhypothesis{parameter_crossvalidation}{parameter\_crossvalidation}{231,76,60}{For each of the tasks, we executed the different methods and trained their parameters using the metrics specified by @metrics}{Train parameters for each of the TI tasks either by generating a grid of parameters or by using a smarter parameter optimisation algorithm}{}{}

  \myhypothesis{benchmarking}{benchmarking}{164,204,46}{We benchmarked TI methods using the real and simulated data and constructed a set of guidelines for the user to follow}{We need to:\\
    * generate synthetic data @generate\_synthetic\_data\\
    * perform parameter cross-validation @parameter\_crossvalidation\\
    * find methods (\& parameters) which work well for certain datasets/task characteristics @characteristics\_vs\_performance\\
    * construct a set of guidelines for the reader with which to decide which method to use @construct\_guidelines}{}{}

  \myhypothesis{characteristics_vs_performance}{characteristics\_vs\_performance}{164,204,46}{We could link performance to dataset and method metadata}{Use dataset and method metadata to determine which methods and parameters work well for certain TI tasks}{We hope to see links between the performance and the metadata}{We need to do this in order to start constructing guidelines}

  \myhypothesis{construct_guidelines}{construct\_guidelines}{164,204,46}{We constructed a set of usage guidelines}{Attempt to construct TI usage guidelines depending on the TI task.}{This will likely not be feasible, but would be extremely useful.}{Guide the biologists in their selection of methods\\
    Guide the bio-informatician in the choice of future methods}

  \myhypothesis{biological_validation}{biological\_validation}{164,204,46}{Results from real and simulated data are similar, validating the simulated data, especially on more complex tasks for which no decent real datasets are available. The selected datasets cover the range of TI tasks as well as possible.}{Apply guidelines on real data and show that the selected methods work well in comparison to when the guidelines are not followed. A TI method that obtains a good relative performance on synthetic data should also obtain a good performance on the biological data}{}{}

  \myhypothesis{future}{future}{254,179,8}{We also look to the future of single-cell technologies and TI}{We simulate data which will be available soon\\
    We also generate data which can't be modelled yet by current methods}{N/A}{We have only seen the tip of the iceberg of which can be modeled using current TI methods, and we thus stimulate the development of future methods and the generation of data from more complex biological settings}

  \myhypothesis{simulate_inferred_networks}{simulate\_inferred\_networks}{254,179,8}{We can generate expression data on inferred networks of developing cells}{0. Collect expression of cells in a dynamic process\\
    1. Perform NI\\
    2. Generate expression of GRN\\
    3. Apply TI on real expression data\\
    4. Apply TI on synthetic expression data\\
    5. TI results on real expression data should be comparable to synthetic expression data}{Data looks similar, as long as we can trust the NI}{It is a validation of the whole synthetic data generation workflow}

  \myhypothesis{tradeoff_cost_performance}{tradeoff\_cost\_performance}{254,179,8}{We compared current platforms, estimated the cost to improve certain parameters, and predicted which parameters to improve to obtain the best performance increase versus cost increase}{We should attempt to quantify the cost associated with improving each of the sources of technical noise, and make a trade-off between TI performance and total cost}{}{scRNA-seq platforms will improve. We can find out what the most cost-effective way of choosing/improving a platform is in order to study dynamic processes.}

  \myhypothesis{mrna_vs_protein}{mrna\_vs\_protein}{254,179,8}{Observing protein levels might improve TI}{Add flowcyto and proseq platforms. Evaluate}{As cells differentiate, mRNA levels increase first, protein levels increase later.\\
    Observing protein expression levels might thus improve TI.}{Integrative TI with index sorting}

  \myhypothesis{multiple_processes}{multiple\_processes}{254,179,8}{Most existing methods will have problems detecting multiple processes. These might be synchronised or not.}{Generate networks and expression data containing multiple dynamic processes}{Watch TI methods fail horribly}{Promoting the development of better methods}

  \myhypothesis{intercellular_communication}{intercellular\_communication}{254,179,8}{We generated data in which multiple cells interact with eachother, influencing eachothers expression. And develop methods to synchronize multiple cells}{Add networks between cells\\
    Slightly different metric to comparison}{✓}{scRNA-seq will in the future allow the inference of networks between cells}

  \myhypothesis{modular_ti}{modular\_ti}{254,179,8}{Each TI approach can be disassembled into smaller components, new TI approaches can be constructed from a combination from components}{TODO}{}{}

  \myhypothesis{communication}{communication}{155,89,182}{We communicated our results to a broad audience in an interesting manner}{TODO}{}{}

  \myhypothesis{manuscript}{manuscript}{155,89,182}{We communicated to the 100 most interested people}{Write a clear and consise manuscript}{}{}

  \myhypothesis{large_communication}{large\_communication}{155,89,182}{We communicated to the 1000 most interested people}{We make our evaluation code available as a package.\\
    We post the results on the github page. When then evaluation pipeline is modified (i.e. new methods or datasets have been added), the results are also updated}{}{}

  \myhypothesis{mass_communication}{mass\_communication}{155,89,182}{We communicated to 10000+ people}{Go to conferences and/or contact the press?}{Fame and glory}{}
