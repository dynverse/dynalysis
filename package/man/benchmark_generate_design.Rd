% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/suite_benchmark_generate_design.R
\name{benchmark_generate_design}
\alias{benchmark_generate_design}
\title{Generate a benchmarking design}
\usage{
benchmark_generate_design(datasets, methods, parameters = NULL,
  priors = NULL, num_repeats = 1, crossing = NULL)
}
\arguments{
\item{datasets}{The datasets to be used in the evaluation.
Must be a named list consisting of dataset ids (character),
dynwrap::data_wrapper's, or functions that generate dynwrap::data_wrapper's.}

\item{methods}{The methods to be evaluated.
Must be a named list consisting of method_ids (character) or dynwrap::ti_wrapper's.}

\item{parameters}{A named list containing data frames of the parameters to evaluate.
The names of the list must be present in method_ids.
The data frames must be of format \code{data_frame(id = "set1", param1 = "a", param2 = 2.0)}.}

\item{priors}{A list of lists. Each sublist contains a list of priors that each method is allowed to optionally use.
Check \code{\link[dynwrap:priors]{dynwrap::priors}} for a list of possible priors. Default priors given is "none"}

\item{num_repeats}{The number of times to repeat the evaluation.}

\item{crossing}{A data frame containing the combinations of methods, datasets, parameters and priors to be evaluated.
Must be a data frame containing the columns dataset_id, method_id, prior_id, repeat_ix and param_id}
}
\description{
Generate a benchmarking design
}
\examples{
\dontrun{
library(tibble)

datasets <- list(
  "synthetic/dyntoy/bifurcating_1",
  dyntoy::generate_dataset(id = "test1"),
  test2 = function() dyntoy::generate_dataset()
)

methods <- list(
  "comp1",
  dynmethods::ti_scorpius(),
  "tscan"
)

parameters <- list(
  scorpius = tibble(
    id = c("test1", "default"),
    params = list(
      list(ndim = 5),
      list()
    )
  ),
  tscan = tibble(
    id = paste0("test", 1:3),
    clusternum_lower = 4:6,
    clusternum_upper = 18:20
  )
)

priors <- list(
  list(id = "none"),
  list(id = "some", set = c("start_id", "end_n"))
)

benchmark_generate_design(
  datasets = datasets,
  methods = methods,
  parameters = parameters,
  priors = priors,
  num_repeats = 2
)
}
}
