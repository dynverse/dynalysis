#' Generate a benchmarking design
#'
#' @param dataset_ids The ids of the datasets to be used in the evaluation.
#' @param method_ids The ids of the methods to be evaluated.
#' @param parameters A named list containing data frames of the parameters to evaluate.
#'   The names of the list must be present in method_ids.
#'   The data frames must correspond to the parameter set \code{par_set} of each method,
#'   e.g. as generated by \code{\link[ParamHelpers]{generateDesignOfDefaults}},
#'   but must also contain the column \code{paramset_ix}.
#' @param num_repeats The number of times to repeat the evaluation.
#' @inheritParams dynwrap::infer_trajectories
#'
#' @examples
#' generate_benchmark_design(
#'   dataset_ids = c("toy/bifurcating_1", "toy/bifurcating_2"),
#'   method_ids = c("angle", "scorpius", "tscan"),
#'   parameters = list(scorpius = tibble(paramset_ix = 1), tscan = tibble(paramset_ix = 1:3, clusternum_lower = 4:6, clusternum_upper = 18:20)),
#'   give_priors = NULL,
#'   num_repeats = 2
#' )
#' @export
generate_benchmark_design <- function(
  dataset_ids,
  method_ids,
  parameters = map(method_ids, ~tibble(paramset_ix = 1)),
  give_priors = NULL,
  num_repeats = 1
) {
  testthat::expect_true(is.character(dataset_ids))
  testthat::expect_true(is.character(method_ids))
  testthat::expect_true(all(names(parameters) %in% method_ids))
  testthat::expect_true(is.list(parameters))
  testthat::expect_true(is.numeric(num_repeats))

  # generate designs of the different parts of the evaluation
  methods_design <- map2_dfr(method_ids, parameters[method_ids], function(method_id, parameters) {
    if (is.null(parameters)) {
      parameters <- tibble(paramset_ix = 1)
    }

    tibble(
      method_id = method_id,
      paramset_ix = parameters$paramset_ix,
      parameters = parameters %>% select(-paramset_ix) %>% tmap(as.list)
    )
  })

  datasets_design <- tibble(dataset_id = dataset_ids)

  give_priors_design <- tibble(give_priors = list(give_priors))

  num_repeats_design <- tibble(repeat_ix = 1:num_repeats)

  # all combinations of the different parts
  crossing(
    datasets_design,
    methods_design,
    give_priors_design,
    num_repeats_design
  )
}



#' A benchmark suite with which to run all the methods on the different datasets
#'
#' @param design Design tibble of the experiment:
#'   - A tibble containing columns `dataset_ids`, `method_id`, `parameters`, `give_priors` and `repeat_ix`
#'   - A tibble generated by [generate_benchmark_design()]
#' @param timeout_per_execution The maximum number of seconds each execution is allowed to run, otherwise it will fail.
#' @param max_memory_per_execution The maximum amount of memory each execution is allowed to use, otherwise it will fail.
#' @param metrics Which metrics to evaluate; see \code{\link{calculate_metrics}} for a list of which metrics are available.
#' @param local_output_folder A folder in which to output intermediate and final results.
#' @param remote_output_folder A folder in which to store intermediate results in a remote directory when using the qsub package.
#' @param execute_before Shell commands to execute before running R.
#' @param verbose Whether or not to print extra information.
#'
#' @importFrom pbapply pblapply
#' @importFrom readr read_rds write_rds
#'
#' @export
benchmark_submit <- function(
  design,
  timeout_per_execution = 3600,
  max_memory_per_execution = "10G",
  metrics = "correlation",
  local_output_folder = derived_file(""),
  remote_output_folder,
  execute_before = NULL,
  verbose = TRUE
) {
  requireNamespace("qsub")

  benchmark_submit_check(
    design,
    timeout_per_execution,
    max_memory_per_execution,
    metrics,
    local_output_folder,
    remote_output_folder,
    execute_before,
    verbose
  )

  ## prepare for remote execution; create a qsub config
  qsub_config <- qsub::override_qsub_config(
    wait = FALSE,
    remove_tmp_folder = FALSE,
    stop_on_error = FALSE,
    verbose = FALSE,
    num_cores = 1,
    memory = max_memory_per_execution,
    max_wall_time = timeout_per_execution,
    execute_before = execute_before,
    local_tmp_path = local_output_folder,
    remote_tmp_path = remote_output_folder
  )

  ## run evaluation for each method separately
  run_method <- function(design_method) {
    testthat::expect_equal(length(unique(design_method$method_id)), 1)
    method_id <- design_method$method_id[[1]]

    suite_method_folder <- file.path(local_output_folder, method_id)
    output_file <- file.path(suite_method_folder, "output.rds")
    qsubhandle_file <- file.path(suite_method_folder, "qsubhandle.rds")

    qsub::mkdir_remote(suite_method_folder)

    if (!file.exists(output_file) && !file.exists(qsubhandle_file)) {
      cat("Submitting ", method_id, "\n", sep = "")
    }

    # set parameters for the cluster
    qsub_config_method <-
      qsub::override_qsub_config(
        qsub_config = qsub_config,
        name = paste0("D_", method_id),
        local_tmp_path = paste0(suite_method_folder, "/r2gridengine")
      )

    # which packages to load on the cluster
    qsub_packages <- c("dplyr", "purrr", "dyneval", "readr")

    # which data objects will need to be transferred to the cluster
    qsub_environment <-  c(
      "design_method", "metrics", "verbose"
    )

    # submit to the cluster
    qsub_handle <- qsub::qsub_lapply(
      X = seq_len(nrow(design_method)),
      object_envir = environment(),
      qsub_environment = qsub_environment,
      qsub_packages = qsub_packages,
      qsub_config = qsub_config_method,
      FUN = benchmark_qsub_fun
    )

    # save data and handle to RDS file
    metadata <- lst(
      local_output_folder,
      remote_output_folder,
      design_method,
      timeout_per_execution,
      max_memory_per_execution,
      metrics,
      suite_method_folder,
      output_file,
      qsubhandle_file,
      qsub_handle
    )
    readr::write_rds(metadata, qsubhandle_file)
  }

  invisible()
}

#' @importFrom testthat expect_equal expect_is
#' @importFrom ParamHelpers dfRowToList
benchmark_submit_check <- function(
  design,
  timeout_per_execution,
  max_memory_per_execution,
  metrics,
  local_output_folder,
  remote_output_folder,
  execute_before,
  verbose
) {
  # check datasets
  testthat::expect_true(all(design$dataset_id %in% list_datasets()$dataset_id))

  # check methods
  testthat::expect_is(design$method_id, "character")
  testthat::expect_true(all(design$method_id %in% dynmethods::methods$method_id))

  # check parameters
  testthat::expect_true(all(map_lgl(design$parameters, is.list)))

  walk2(
    dynmethods::methods %>% slice(match(design$method_id, method_id)) %>% pull(inputs),
    design$parameters,
    function(inputs, parameters) {
      testthat::expect_true(all(
        names(parameters) %in% (inputs %>% filter(type == "parameter") %>% pull(input_id))
      ))
    }
  )

  # check timeout_per_execution
  testthat::expect_is(timeout_per_execution, "numeric")

  # check max_memory_per_execution
  testthat::expect_is(max_memory_per_execution, "character")
  testthat::expect_match(max_memory_per_execution, "[0-9]+G")
}

#' Helper function for benchmark suite
#'
#' @param grid_i Benchmark config index
benchmark_qsub_fun <- function(design_row) {
  # call helper function
  benchmark_run_evaluation(design_row, metrics, verbose)
}

#' @importFrom readr read_rds
#' @importFrom ParamHelpers dfRowToList trafoValue
benchmark_run_evaluation <- function(
  design_row,
  metrics,
  verbose
) {
  # read dataset
  dataset_id <- design_row$dataset_id
  dataset <- load_dataset(dataset_id, as_tibble = T)

  # get parameters
  parameters <- design_row$parameters

  # get method
  method_function <- dynmethods::methods %>%
    slice(match(design_row$method_id, method_id)) %>%
    pull(fun_name) %>%
    get("package:dynmethods")
  method <- method_function()

  # start evaluation
  out <- evaluate_ti_method(
    datasets = datasets,
    method = method,
    parameters = parameters,
    metrics = metrics,
    extra_metrics = NULL,
    output_model = TRUE,
    mc_cores = 1,
    verbose = verbose
  )

  # create summary
  bind_cols(
    grid[grid_i,] %>% select(-dataset_id),
    tibble(
      grid_i,
      params_notrafo = list(parm_df),
      params_trafo = list(parm_list),
      model = out$.models
    ),
    out$.summary %>%
      mutate(error_message = ifelse(is.null(error[[1]]), "", error[[1]]$message)) %>%
      select(-error)
  )
}

#' Fetch the results of the benchmark jobs from the cluster.
#'
#' @param local_output_folder The folder in which to output intermediate and final results.
#'
#' @importFrom readr read_rds write_rds
#' @export
benchmark_fetch_results <- function(local_output_folder) {
  requireNamespace("qsub")

  method_names <- list.dirs(local_output_folder, full.names = FALSE, recursive = FALSE) %>% discard(~ . == "")

  # process each method separately
  map(method_names, function(method_name) {
    method_folder <- paste0(local_output_folder, "/", method_name)
    output_metrics_file <- paste0(method_folder, "/output_metrics.rds")
    output_models_file <- paste0(method_folder, "/output_models.rds")
    qsubhandle_file <- paste0(method_folder, "/qsubhandle.rds")

    # if the output has not been processed yet, but a qsub handle exists,
    # attempt to fetch the results from the cluster
    if (!file.exists(output_metrics_file) && file.exists(qsubhandle_file)) {

      cat(method_name, ": Attempting to retrieve output from cluster: ", sep = "")
      metadata <- readr::read_rds(qsubhandle_file)
      grid <- metadata$grid
      qsub_handle <- metadata$qsub_handle
      num_datasets <- qsub_handle$num_datasets

      # attempt to retrieve results; return NULL if job is still busy or has failed
      output <- qsub::qsub_retrieve(
        qsub_handle,
        wait = FALSE
      )

      if (!is.null(output)) {
        cat("Output found! Saving output.\n", sep = "")

        suppressWarnings({
          qacct_out <- qsub::qacct(qsub_handle)
        })

        # process each job separately
        outputs <- map_df(seq_len(nrow(grid)), function(grid_i) {
          out <- output[[grid_i]]

          if (length(out) != 1 || !is.na(out)) {
            # hooray, the benchmark suite ran fine!
            out

          } else {
            # if qacct is empty or the correct datasetid cannot be found, then
            # this job never ran
            if (is.null(qacct_out) || !any(qacct_out$datasetid == grid_i)) {
              qsub_error <- "Job cancelled by user"
            } else {
              qacct_filt <- qacct_out %>% filter(datasetid == grid_i) %>% arrange(desc(row_number_i)) %>% slice(1)

              qacct_memory <- qacct_filt$maxvmem %>% str_replace("GB$", "") %>% as.numeric
              qacct_exit_status <- qacct_filt$exit_status %>% str_replace(" +", " ")
              qacct_exit_status_number <- qacct_exit_status %>% str_replace(" .*", "")
              qacct_user_time <- qacct_filt$ru_stime %>% str_replace("s$", "") %>% as.numeric

              qsub_memory <- qsub_handle$memory %>% str_replace("G$", "") %>% as.numeric
              qsub_user_time <- qsub_handle$max_wall_time

              memory_messages <- c("cannot allocate vector of size", "MemoryError")
              is_memory_problem <- function(message) {
                any(map_lgl(memory_messages, ~grepl(., message)))
              }

              qsub_error <-
                if (qacct_exit_status_number %in% c("134", "139") || is_memory_problem(attr(out, "qsub_error"))) {
                  "Memory limit exceeded"
                } else if (qacct_exit_status_number %in% c("137", "140", "9", "64")) {
                  "Time limit exceeded"
                } else if (qacct_exit_status_number != "0") {
                  qacct_exit_status
                } else {
                  attr(out, "qsub_error")
                }
            }

            # create a method that will just return the error generated by qsub
            method_failer <- metadata$method
            method_failer$run_fun <- function(...) {
              stop(qsub_error)
            }
            formals(method_failer$run_fun) <- formals(metadata$method$run_fun)

            # "rerun" the evaluation, in order to generate the expected output except with
            # the default fail-scores for each of the metrics
            out <- benchmark_run_evaluation(
              grid = grid,
              grid_i = grid_i,
              parms = metadata$parms,
              method = method_failer,
              metrics = metadata$metrics,
              verbose = FALSE
            )
          }
        })

        # save models separately
        models <- outputs$model
        model_ids <- map_chr(models, function(model) {
          if (!is.null(model)) {
            model$id
          } else {
            NA
          }
        })
        models <- models %>% set_names(model_ids)
        outputs <- outputs %>% select(-model) %>% mutate(model_i = seq_len(n()), model_id = model_ids)
        readr::write_rds(models, output_models_file)

        # save output
        readr::write_rds(outputs, output_metrics_file)

      } else {
        # the job is probably still running
        suppressWarnings({
          qstat_out <- qsub::qstat_j(qsub_handle)
        })

        error_message <-
          if (is.null(qstat_out) || nrow(qstat_out) > 0) {
            "job is still running"
          } else {
            "qsub_retrieve of results failed -- no output was produced, but job is not running any more"
          }

        cat("Output not found. ", error_message, ".\n", sep = "")
      }

      NULL
    } else {
      if (file.exists(output_metrics_file)) {
        cat(method_name, ": Output already present.\n", sep = "")
      } else {
        cat(method_name, ": No qsub file was found.\n", sep = "")
      }
      NULL
    }

  })

  # return nothing
  invisible()
}


#' Gather and bind the results of the benchmark jobs
#'
#' @param local_output_folder The folder in which to output intermediate and final results.
#' @param load_models Whether or not to load the models as well.
#'
#' @importFrom readr read_rds
#' @export
benchmark_bind_results <- function(local_output_folder, load_models = FALSE) {
  method_names <- list.dirs(local_output_folder, full.names = FALSE, recursive = FALSE) %>% discard(~ . == "")

  # process each method separately
  as_tibble(map_df(method_names, function(method_name) {
    method_folder <- paste0(local_output_folder, method_name)
    output_metrics_file <- paste0(method_folder, "/output_metrics.rds")
    output_models_file <- paste0(method_folder, "/output_models.rds")

    if (file.exists(output_metrics_file)) {
      cat(method_name, ": Reading previous output\n", sep = "")
      output <- readr::read_rds(output_metrics_file)

      # read models, if requested
      if (load_models && file.exists(output_models_file)) {
        models <- readr::read_rds(output_models_file)
        output$model <- models
      }

      output
    } else {
      cat(method_name, ": Output not found, skipping\n", sep = "")
      NULL
    }
  }))
}
