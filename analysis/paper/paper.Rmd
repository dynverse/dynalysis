---
title: "A comparison of single-cell trajectory inference methods: towards more accurate and robust tools"
author:
  - Wouter Saelens* ¹ ²
  - Robrecht Cannoodt* ¹ ² ³
  - Helena Todorov¹ ² ⁴ ⁵
  - Yvan Saeys¹ ²
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: references.bib
csl: nature-biotechnology.csl
abstract: |
  Using single-cell transcriptomics data, it is now possible to computationally order cells along trajectories, allowing the unbiased transcriptome-wide study of cellular dynamic processes. Since 2014, already dozens of trajectory inference methods have been developed, each with its own set of methodological characteristics. This makes a direct comparison between trajectory inference methods often difficult, and as a result a comprehensive assessment of the performance and robustness of each method is still lacking. We developed a framework of data structures for trajectory inference methods in order to make their results comparable to each other and to a gold standard, when available. In this work, we compare the results from a total of 22 trajectory inference methods, on a large collection of real datasets and realistic synthetic datasets. We employ a novel parameter optimisation procedure, and compare methods using several metrics, including accuracy of the inferred ordering, correctness of the network wiring, code quality and user friendliness. We conclude with practical guidelines for method users, and also believe our evaluation framework can be used to spearhead the development for new methods. Our evaluation pipeline is fully reproducible, can be easily extended, and is available at www.github.com/dynverse.
keywords: |
  trajectory inference; pseudotemporal ordering; single-cell transcriptomics
highlights: |
  These are the highlights. 
---
\* Equal contribution  
¹ Data mining and modelling for biomedicine, VIB Center for Inflammation Research, Ghent, Belgium.  
² Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium.  
³ Center for Medical Genetics, Ghent University Hospital, Ghent, Belgium.  
⁴ CIRI, Centre International de Recherche en Infectiologie, Inserm, Lyon, France.  
⁵ Université Claude Bernard Lyon 1, Lyon, France.  
<!----
Tips for working in this document:
* Go to tools -> perferences and remove Use smart quotes
* Add extra enters after every paragraph (this is markdown!)


TITLES: 
Not all trajectory inference methods are created equal
Trajectory inference: work in progress
Trajectory inference: we’re not quite there yet
Paving the way for trajectories
Benchmarking trajectory inference methods
Bringing single-cell trajectory inference methods to the next level
Resolving disarray amongst trajectory inference methods with a comprehensive evaluation
A comparison of trajectory inference methods
Trajectory inference 2.0
We’re on the trajectory to better trajectory inference methods
On the road to more accurate and robust single-cell trajectory inference methods
Towards more accurate and robust single-cell trajectory inference methods
On the trajectory to more accurate,robust and trajectory-like single-cell trajectory inference method trajectories
A comparison of single-cell trajectory inference methods: towards more accurate and robust tools
--->


```{r setup, include=FALSE}
source("setup.R")
```


# Introduction


Single-cell -omics technologies now make it possible to more accurately model biological systems than ever before. One area where single-cell data can be particularly useful is in the study of cellular dynamic processes, such as the cell cycle, cell differentiation and cell activation. When cells are sampled from a population in which cells are at different unknown points in the dynamic process, trajectory inference methods can be used to computationally order these cells along their dynamic process [@cannoodt_computational_2016]. Because they offer an unbiased and transcriptome-wide understanding of the dynamic process, trajectories then allow the identification of new (primed) subsets of cells, delineation the exact wiring of a differentiation tree and inference of regulatory interaction responsible for a bifurcation refs.


```{r}
add_sfig({
n_methods_over_time <- readRDS(figure_file("n_methods_over_time.rds", experiment_id="method_characteristics"))
trajectory_components_over_time <- readRDS(figure_file("trajectory_components_over_time.rds", experiment_id="method_characteristics"))


cowplot::plot_grid(plotlist=list(n_methods_over_time, trajectory_components_over_time), nrow=2, rel_heights = c(0.7, 0.3), labels="auto")
}, 
"methods_over_time", 
"New TI methods over time. **a** Number of methods published or in preprint. **b** Number of methods which can handle a particular type of trajectory.
", 
15, 
10
)
```


<!--- history of Ti, different methods --->




<!--- Common framework of TI, different components, naturally progressing towards the need for an evaluation --->
Although trajectory inference methods use a variety of algorithms and subcomponents to reach a final ordering, most consist of the following three steps: (i) preprocessing (normalization, filtering of genes and/or cells), (ii) conversion to a simplified representation using dimensionality reduction, clustering or graph building and (iii) ordering the cells along the simplified representation [@cannoodt_computational_2016]. Depending on the way the cells are ordered, some prior information about the dynamic process can be optionally used or required by the method, which can both bias the trajectory to current knowledge but also guide the method towards choosing the right trajectory among many others. Furthermore, with the ongoing strong scalability in single-cell -omics technologies, it becomes more and more important that analysis methods become scalable and user friendly .


<!--- Common framework of TI, different components, naturally progressing towards the need for an evaluation --->
Given this plethora of available trajectory inference methods, it is important that methods are compared, so that users can use the most optimal method available for their problem. Moreover, new methods need to be rigorously tested, so that development can focus on improving the current state-of-the-art. In this study, we therefore for the first time developed a comprehensive evaluation framework for trajectory inference methods. We test both on synthetic data, for which the gold standard is known, and real data, for which in some cases the gold standard can be extracted from expert knowledge. We include a rigorous parameter optimisation, making sure that parameters are optimized while avoiding overfitting on characteristics of specific datasets. To make our framework reusable and extendable, we make use of continuous analysis [@beaulieu-jones_reproducibility_2017], which allows the developers of new methods to easily test their methods and compare them against the state-of-the-art.


# Results
## Method characterization


```{r}
# Load method characteristics
methods <- readRDS(derived_file("methods.rds", experiment_id="method_characteristics"))
```


We gathered a list of `r sum(methods$is_ti, na.rm=TRUE)` TI methods from literature, and selected a subset of `r sum(methods$evaluated)` for evaluation, primarily based on their free availability and the presence of a programming interface `r ref("table", "methods")`. We characterized all methods in four different ways: possible trajectory structures, implementation quality, prior information and the underlying algorithm (`r ref("fig", "method_characteristics")`).


`r ref("table", "methods", anchor=TRUE)` Overview of current trajectory inference methods, and whether they were included in this current evaluation study
```{r}
read_rds(figure_file("methods_table.rds", "method_characteristics"))
```


At the level of trajectory structures, most recent methods can handle one or more splits (either bifurcations or multifurcations) in the trajectory (`r ref("fig", "method_characteristics", "a")`). However, only a handful of more recent methods can currently handle more complex graph structures which can include loops (`r ref("sfig", "methods_over_time", "b")`), one of which is included in the current evaluation (SLICER).


While not directly related to the accuracy of the inferred trajectory, the quality of the implementation is an important first evaluation metric, because good unit testing makes sure the implementation is correct, good documentation makes it easier for potential users to apply the method on their data, and overall good code quality makes it possible for other developers to adapt the method and extend it further. We therefore looked at the implementation of each method, and assessed their quality using a transparent scoring scheme (`r ref("stable", "qc_checks")`). The individual quality checks can be grouped in two ways: what aspect of the method they investigate (availability, code quality, code assurance, documentation, behaviour and the paper) or for which purpose(s) they are important (user friendliness, developer friendliness or good science). These categorisations can then be used to help current developers to improve their tool, and to guide the selection of users and developers to use these tools for their purpose. After publishing this preprint, we will contact the authors of each method, allowing them to improve their method before the final publishing of the evaluation.


```{r}
add_stable(
read_rds(figure_file("qc_checks.rds","method_characteristics")), 
"qc_checks", 
""
)
```


Most methods fulfilling most of the basic criteria, such as free availability and basic code quality criteria (`r ref("fig", "method_characteristics", "b")`). Recent methods tend to have higher code quality (`r ref("sfig", "qc_over_time")`). However, several aspects are consistently lacking for the majority of the methods (`r ref("fig", "method_characteristics", "e")`) and we believe that these should receive extra attention from developers. Although these outstanding issues cover all five categories, code assurance and documentation in particular are problematic areas (`r ref("sfig", "method_qc_check")` and `r ref("fig", "method_characteristics", "c")`), notwithstanding studies pinpointing these as good practices [@wilson_best_2014].


The underlying algorithms can be divided into several modules, and these frequently contain soms clustering, dimensionality reduction and/or graph building steps. Interestingly, modules are frequently shared between different algorithms (`r ref("sfig", "method_components")`).


`r anchor("fig", "method_characteristics")`
![](`r figure_file("method_characteristics.svg", experiment_id="method_characteristics")`)
`r ref("fig", "method_characteristics", anchor=TRUE)` First characterization of each method.


```{r}
add_sfig({
readRDS(figure_file("method_qc_check.rds", experiment_id="method_characteristics"))
}, 
"method_qc_check", 
"Caption", 
10, 
13
)
```


```{r}
add_sfig({
readRDS(figure_file("method_components.rds", experiment_id="method_characteristics"))
}, 
"method_components", 
"TI methods frequently share steps", 
15, 
7.5
)
```




```{r}
add_sfig({
readRDS(figure_file("qc_over_time.rds", experiment_id="method_characteristics"))
}, 
"qc_over_time", 
"Quality control scores and number of citations over time", 
10, 
10
)
```
### Evaluation overview
`r anchor("fig", "evaluation_overview")`
![](`r figure_file("ti_evaluation_overview_v1.svg", experiment_id="evaluation_overview")`)
`r ref("fig", "evaluation_overview")` Overview of the evaluation


Our evaluation was structured in three main phases. In the first phase, we used toy data as a positive control for each method, assessing whether it can detect the correct trajectories in very simplistic data. Next, we generated synthetic data from a model of gene regulation, combined with a simulation of the technical variation introduced by single-cell RNA sequencing. We used this synthetic data in a parameter optimization procedure, where we used cross-validation between datasets to avoid overfitting on particular data set structures. Finally, in the third phase, we used the optimized parameters to score the performance of each method on real data. We split the real datasets in two groups based on the type of reference trajectory available. The reference in datasets with a silver standard were defined by the author’s based on the expression data itself, while the reference for gold standard was defined using external information, such as fluorescent sorting or time series.[a]
### Score metrics and parameter optimisation


## Overall method comparison


## Evaluation on synthetic data
Initial evaluation..
Robustness..
Gegenereerde synthetische data. Parameter tuning voor echte data + initiële evaluatie (tot wat is de methode in staat) + robustness etc testing


## Evaluation on real data
Biological relevance..
Echte data. Evaluatie (biologische relevantie van de methode)


# Discussion
# Conclusion


# Online Methods


## Trajectory inference methods


We gathered a list of `r sum(methods$is_ti, na.rm=TRUE)` trajectory inference methods `r ref("table", "methods", anchor=TRUE)`, based on two existing lists found online [@davis_awesomesinglecelllistsoftware_2018,@gitter_singlecellpseudotimeoverviewalgorithms_2018] and by searching in literature for "trajectory inference" and "pseudotemporal ordering". We believe this to be the most complete list to data, but are happy to include additional methods through e-mail or an issue on our github (https://github.com/dynverse/dynalysis). Methods were excluded from the evaluation based on several criteria: `r glue::collapse(glue::glue("({non_inclusion_reasons$footnote}) {non_inclusion_reasons$long}"), ", ")`. This resulted in the inclusion of `r sum(methods$evaluated)` methods in the evaluation (`r ref("table", "methods")`).


## Method quality control


We created a transparent scoring scheme to check the quality of each method (`r ref("table", "methods")`) and used several references to weigh each quality check.


## Synthetic data generation


Our workflow to generate synthetic data is based on the well established workflow used in the evaluation of network inference methods [@marbach_wisdom_2012] and consists of four main steps: network generation, simulation, gold standard extraction and simulation of the scRNA-seq experiment. At every step, great care was taking to mimic real cellular regulatory networks as best as possible, while keeping the model simple and easily extendable.


### Network generation


One of the main process involved in cellular dynamic processes is through gene regulation, where regulatory cascades and feedback loops lead to . The exact way a cell chooses a certain path during its differentiation is still an active research field, although certain models have already emerged and been tested in vivo. One driver of bifurcations seem to be mutual antagonism, where two genes [@XuRegulationbifurcatingcell2015] or gene modules strongly inhibit each other, forcing one of the two to become inactive [@GrafForcingcellschange2009]. Which of these two Of Modules? [Th17]


## Real data processing


We gathered 


## Normalization
## Evaluation metrics
## Parameter optimization


# Acknowledgements


# Supplementary


```{r, echo=FALSE, results='asis'}
source("analysis/paper/sfigs_stables.R")
```


# Colophon


This report was generated on `r Sys.time()` using `r devtools:::platform_info()$version` and the following packages:
```{r colophon, cache = FALSE}
devtools::session_info()$packages %>% knitr::kable()
```


The current Git commit details are:


```{r}
# what commit is this file at? You may need to change the path value
# if your Rmd is not in analysis/paper/
git2r::repository(".")
```


# References 


[a]Outdated