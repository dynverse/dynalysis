---
title: "A comparison of single-cell trajectory inference methods: towards more accurate and robust tools"
bibliography: references.bib
csl: nature-biotechnology.csl
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage[table]{xcolor}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{tabu}
  - \usepackage[normalem]{ulem}
  - \usepackage{lscape}
  - \ExecuteBibliographyOptions{sorting=none,sortcites=true,autocite=superscript}
mainfont: Open Sans


---


*`r format(Sys.time(), '%d %B, %Y')`*
<!--
Tips for working in this document:
* Go to tools -> perferences and remove Use smart quotes
* Add extra enters after every paragraph (this is markdown!)
* Enable automatic zotero syncing to bibtex
* Make sure to pin the zotero bibtex references after first using them!


TITLES: 
Not all trajectory inference methods are created equal
Trajectory inference: work in progress
Trajectory inference: we’re not quite there yet
Paving the way for trajectories
Benchmarking trajectory inference methods
Bringing single-cell trajectory inference methods to the next level
Resolving disarray amongst trajectory inference methods with a comprehensive evaluation
A comparison of trajectory inference methods
Trajectory inference 2.0
We’re on the trajectory to better trajectory inference methods
On the road to more accurate and robust single-cell trajectory inference methods
Towards more accurate and robust single-cell trajectory inference methods
On the trajectory to more accurate,robust and trajectory-like single-cell trajectory inference method trajectories
A comparison of single-cell trajectory inference methods: towards more accurate and robust tools
-->








```{r setup, include=FALSE}
source(paste0(dynalysis::get_dynalysis_folder(), "/analysis/paper/setup.R"))
```
`r glue::collapse(sample(c("Wouter Saelens* ¹ ²", "Robrecht Cannoodt* ¹ ² ³")), ", ")`, Helena Todorov¹ ² ⁴ ⁵, Yvan Saeys¹ ²


\* Equal contribution  
¹ Data mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium.  
² Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium.  
³ Center for Medical Genetics, Ghent University Hospital, Ghent, Belgium.  
⁴ CIRI, Centre International de Recherche en Infectiologie, Inserm, Lyon, France.  
⁵ Université Claude Bernard Lyon 1, Lyon, France.  


# Abstract


Using single-cell transcriptomics data, it is now possible to computationally order cells along trajectories, allowing the unbiased transcriptome-wide study of cellular dynamic processes. Since 2014, more than 50 trajectory inference methods have been developed, each with its own set of methodological characteristics. As a result, comparing between trajectory inference methods is often challenging, and a comprehensive assessment of the performance and robustness of each method is still lacking. We developed a common data structure for trajectory inference methods in order to make their results comparable to each other and to a gold standard. Using this framework, we compare the results from a total of 22 trajectory inference methods, on a large collection of real datasets and realistic synthetic datasets. We employ several metrics, including accuracy of the inferred ordering, correctness of the network topology, code quality and user friendliness. We conclude with practical guidelines for method users, and also believe our evaluation framework can be used to spearhead the development for new methods. Our evaluation pipeline is fully reproducible, can be easily extended, and is available at https://www.github.com/dynverse.


This is the first comprehensive assessment the performance of trajectory inference methods publicly available online. Comments and criticisms are welcome.


# Introduction
<!-- Introduction to single-cell and TI --->


Single-cell -omics technologies now make it possible to model biological systems more accurately than ever before. One area where single-cell data can be particularly useful is in the study of cellular dynamic processes, such as the cell cycle, cell differentiation and cell activation. When cells are sampled from a population in which cells are at different unknown points in the dynamic process, trajectory inference (TI) methods can be used to computationally order these cells along their dynamic process [@cannoodt_computational_2016]. TI methods offer an unbiased and transcriptome-wide understanding of dynamic process [@tanay_scaling_2017], and thus allow the objective identification of new (primed) subsets of cells [@schlitzer_identificationcdc1cdc2committed_2015], delineation of a differentiation tree [@velten_humanhaematopoieticstem_2017; @see_mappinghumandc_2017] and inference of regulatory interaction responsible for a bifurcation [@aibar_scenicsinglecellregulatory_2017]. Current applications of TI focus on specific subsets of cells, but current efforts to construct transcriptomic catalogues of whole organisms [@regev_scienceforumhuman_2017; @han_mappingmousecell_2018] underline the urgency for accurate, scalable[@aibar_scenicsinglecellregulatory_2017; @angerer_singlecellsmake_2017] and user-friendly TI methods.


```{r}
add_sfig(
figure_file("methods_timeline.pdf", "4-method_characterisation"),
"methods_over_time", 
"New TI methods over time. **a** Number of methods published or in preprint. **b** Number of methods which can handle a particular type of trajectory."
)
```


<!-- history of Ti, different methods -->
```{r}
fixed_linear <- implementations %>%
 filter(
   fixes_topology != 'free', 
   maximal_trajectory_type == 'directed_linear'
 ) %>%
 .$implementation_id
fixed_bifurcating <- implementations %>%
 filter(
   fixes_topology != 'free', 
   maximal_trajectory_type == 'bifurcation'
 ) %>%
 .$implementation_id
fixed_other <- implementations %>%
 filter(
   fixes_topology != 'free', 
   !maximal_trajectory_type %in% c('directed_linear', 'bifurcation')
 ) %>%
 .$implementation_id
fixed_algorithm <- implementations %>%
 filter(fixes_topology == 'algorithm') %>%
 .$implementation_id
fixed_parameters <- implementations %>%
 filter(fixes_topology == 'parameter') %>%
 .$implementation_id
	

```


Dozens of trajectory inference methods have been developed over the last years, and more are being published almost every month (`r ref("sfig", "methods_over_time", "a")`). Initially, most trajectory inference methods fixed the trajectory topology to a linear `r cite_methods(fixed_linear)`, bifurcating `r cite_methods(fixed_bifurcating)` or other `r cite_methods(fixed_other)` topology, by either algorithmic limitations `r cite_methods(fixed_algorithm)` or through user parameters `r cite_methods(fixed_parameters)`. The focus of these methods was therefore mainly to correctly order the cells along this fixed topology. However, several recent methods now infer both the ordering and the topology at the same time, although some topological limitations are still imposed (`r ref("sfig", "methods_over_time", "b")`). Methods which do not impose limitations on the topology at all are now being developed [@welch_slicer:_2016; @wolf_graph_2017], and it is expected that in the future methods will be able to model even more complex behavior, such as multiple dynamic processes happening at parallel in a single cell or the integration of datasets from different patients [@tanay_scaling_2017; @cannoodt_computational_2016].


<!-- Common framework of TI, different components, naturally progressing towards the need for an evaluation -->


```{r}
uses_priors <- implementations %>% 
  gather(prior_id, prior_usage, !!priors$prior_id) %>% 
  filter(prior_usage %in% c("can_use", "can_root")) %>% 
  pull(implementation_id) %>% unique()


requires_priors <- implementations %>% 
  gather(prior_id, prior_usage, !!priors$prior_id) %>% 
  filter(prior_usage %in% c("required", "required_default")) %>% 
  pull(implementation_id) %>% unique()
	

```


Although trajectory inference methods use a variety of algorithms and steps to reach a final ordering, most consist of the following three main steps: (i) preprocessing (normalization, filtering of genes and/or cells), (ii) conversion to a simplified representation using dimensionality reduction, clustering or graph building and (iii) ordering the cells along the simplified representation [@cannoodt_computational_2016]. Depending on the way the cells are ordered, some prior information about the dynamic process can be optionally used`r cite_methods(uses_priors)` or required`r cite_methods(requires_priors)`, which can both bias the trajectory to current knowledge but also guide the method towards choosing the right trajectory among many others.


<!-- Common framework of TI, different components, naturally progressing towards the need for an evaluation →


Given this plethora of available trajectory inference methods, it is important that methods are compared, so that users can use the most optimal methods available for their problem. Moreover, new methods need to be rigorously tested, so that development can focus on improving the current state-of-the-art. In this study, we therefore present the first comprehensive evaluation framework for trajectory inference methods. We compared `r sum(methods$evaluated)` methods both on synthetic data, for which the gold standard is known, and real data, for which a gold standard has to be extracted from expert knowledge. <!-- We include a rigorous parameter optimisation, making sure that parameters are optimized while avoiding overfitting on characteristics of specific datasets. --> To make our framework reusable and extendable, we make use of continuous analysis [@beaulieu-jones_reproducibility_2017], which allows the developers of new methods to easily test their methods and compare them against the state-of-the-art.


# Results
## Method quality control


We gathered a list of `r sum(methods$is_ti, na.rm=TRUE)` TI methods  (`r ref("fig", "evaluation_overview", "a")`), and selected a subset of `r sum(methods$evaluated)` for evaluation, primarily based on their free availability, the presence of a programming interface, and the time they were published (`r ref("table", "implementations")`). We characterised all methods in four different ways: possible trajectory topologies, implementation quality, prior information and the underlying algorithm (`r ref("fig", "implementation_overview")`).


`r ref("table", "implementations", anchor=TRUE)` Overview of current trajectory inference methods, and whether they were included in this current evaluation study
```{r}
read_rds(figure_file("implementations_table.rds", "4-method_characterisation"))[[params$table_format]]
```


`r anchor("fig", "evaluation_overview")`
![](`r figure_file("ti_evaluation_overview_v2.pdf", experiment_id="evaluation_overview")`)
`r ref("fig", "evaluation_overview")` Overview of the evaluation


```{r}
add_stable(
read_rds(figure_file("datasets.rds","2-dataset_characterisation"))[[params$table_format]], 
"datasets", 
"Real datasets used in this study"
)
```




At the level of trajectory topologies, most recent methods can handle one or more splits (either bifurcations or multifurcations) in the trajectory (`r ref("fig", "implementation_overview", "a")`). However, only a handful, mostly recent, methods can handle more complex graph topologies which can include loops (`r ref("sfig", "methods_over_time", "b")`), one of which is included in the current evaluation (SLICER).


While not directly related to the accuracy of the inferred trajectory, the quality of the implementation is an important first evaluation metric, because good unit testing makes sure the implementation is correct, good documentation makes it easier for potential users to apply the method on their data, and overall good code quality makes it possible for other developers to adapt the method and extend it further. We therefore looked at the implementation of each method, and assessed their quality using a transparent scoring scheme (`r ref("stable", "qc_checks")`). The individual quality checks can be grouped in two ways: what aspect of the method they investigate (availability, code quality, code assurance, documentation, behaviour and the paper) or which purpose(s) they serve (user friendliness, developer friendliness or good science). These categorisations can help current developers to improve their tool, and to guide the selection of users and developers to use these tools for their purpose. After publishing this preprint, we will contact the authors of each method, allowing them to improve their method before the final publishing of the evaluation.


```{r}
add_stable(
read_rds(figure_file("qc_checks.rds","4-method_characterisation"))[[params$table_format]], 
"qc_checks", 
"QC checks"
)
```


`r anchor("fig", "implementation_overview")`
![](`r figure_file("implementation_overview.pdf", experiment_id="4-method_characterisation")`)
`r ref("fig", "implementation_overview", anchor=TRUE)` First characterization of each method.




Most methods fulfill most of the basic criteria, such as free availability and basic code quality criteria (`r ref("fig", "implementation_overview", "b")`). Recent methods tend to have higher code quality (`r ref("sfig", "qc_over_time")`). However, several aspects are consistently lacking for the majority of the methods (`r ref("fig", "implementation_overview", "e")`) and we believe that these should receive extra attention from developers. Although these outstanding issues cover all five categories, code assurance and documentation in particular are problematic areas (`r ref("sfig", "implementation_qc_check")` and `r ref("fig", "implementation_overview", "c")`), notwithstanding several studies pinpointing these as good practices [@wilson_best_2014; @artaza_top10metrics_2016].


```{r}
add_sfig(
figure_file("qc_over_time.pdf","4-method_characterisation"), 
"qc_over_time", 
"Quality control scores and number of citations over time"
)
```


The underlying algorithms can be divided into several steps, and these frequently contain soms clustering, dimensionality reduction and/or graph building. Interestingly, steps are frequently shared between different algorithms (`r ref("sfig", "method_components")`). MST in particular, used in the first single-cell RNA-seq trajectory inference methods [@trapnell_dynamicsregulatorscell_2014], is used in almost half of all methods we evaluated  (`r ref("sfig", "method_components", "b")`).


```{r}
add_sfig(
figure_file("method_components.pdf","4-method_characterisation"), 
"method_components", 
"TI methods frequently share steps"
)
```
## Evaluating the accuracy of the inferred trajectory


We compared the accuracy of each method by comparing its output with a gold standard (`r ref("fig", "evaluation_overview", "b")`). To make the heterogenous output of each method comparable, we created wrappers around each method so that it was converted into a common probabilistic model (`r ref("fig", "evaluation_overview", "e")` and `r ref("sfig", "wrapping")`. In this model the overall topology is represented by a network between "milestones", and the cells are placed within the space formed by each set of connected milestones (`r ref("fig", "evaluation_overview", "c")`). This common trajectory format allowed us to compare the output of each method against gold standard trajectories. We compared the results of each TI method with both synthetic datasets, which offer the most exact gold standard, and real datasets, which offer the highest biological relevance. These synthetic datasets were generated using a realistic model of gene regulation [@schaffter_genenetweaversilicobenchmark_2011], and for each synthetic dataset a real dataset was used as reference to match the statistical properties of both datasets as best as possible (`r ref("fig", "evaluation_overview", "d")`). We used both large and smaller real datasets, from a variety of platforms, organisms and containing all different trajectory types (`r ref("fig", "evaluation_overview", "f")`). We further divided the datasets into gold and silver standard, depending on whether the gold standard was extracted from only external knowledge (gold) or including the expression data itself (silver). To compare the predictions of a method with a gold or silver standard, we developed three different score metrics, each assessing a different aspect of the trajectory: the cell ordering, the cell neighbourhood and topology (`r ref("fig", "evaluation_overview", "f")`). To calculate a final ordering, we combined metrics and datasets so that each metric, trajectory type and dataset source had equal weight.


```{r}
add_sfig({
        figure_file('wrapping.pdf', '4-method_characterisation')
}, 
"wrapping", 
"Our wrapping workflow", 
15, 
7.5
)
```


When looking across all datasets, we find that slingshot predicts the most accurate trajectories across different data sources, trajectory types and metrics. Over time, new methods 


Top methods tend to contain similar components, with …


Over time, both the average and maximum performance increased with the addition of new methods `r ref("sfig", "top_performers_over_time")` 






# Discussion
<!-- Some intro -->
Trajectory inference is unique among most other categories of single-cell analysis methods, such as clustering, normalisation and differential expression, because it models the data in a way that was almost impossible using bulk data. Indeed, for no other single-cell analysis types have so many tools been developed, according to omictools.org [@henry_omictoolsinformativedirectory_2014] (https://omictools.com/single-cell-rna-seq-category), the "awesome single cell software" list created by Sean Davis [@davis_awesomesinglecelllistsoftware_2018] and scRNA-tools [@zappia_exploringsinglecellrnaseq_2017] (https://www.scrna-tools.org) . It is therefore critical that these methods, now reaching `r sum(methods$is_ti, na.rm=TRUE)`, are evaluated to guide the users in their choice. In this preprint, we summarise the initial version of our evaluation of these methods, focusing on the quality control and the accuracy of their model using their default parameters on real and synthetic data.


<!-- Results  -->


<!-- Our common output model, advantages & limitations -->
We managed to wrap the output of all methods into one common format. This not only allowed us to compare different methods with a gold standard, but could also be useful for users of these tools because it allows the user to test multiple methods without manual conversion of input and output. Furthermore, it makes it possible to compare the output of different methods, which opens up possibilities for new comparative visualisation techniques or ensemble methods. However, we acknowledge that our model has some limitations. It can currently not take into account uncertainty of the cell's position. This can be both locally, for example uncertainty of its position within one branch, or globally, for example uncertainty over multiple branches. Some current methods already model this uncertainty in some way, mainly locally `r cite_methods("ouija")`, and in the future we will adapt our output model to also allow this uncertainty.


<!----
Limitations of the model:
* Robrecht.Cannoodt @robrechtc 10:21
en heterogene output van alle methode... true, maar sommigen hebben we wel wat moeten tweaken 😛
en ik kan me output bedenken dat niet in ons model zou kunnen passen
* Wouter Saelend @wouters 10:21
oh 😛
* Robrecht.Cannoodt @robrechtc 10:21
welja
* Wouter Saelend @wouters 10:21
Misschien een kritiekpunt is onzekerheid... maarja 😛
* Robrecht.Cannoodt @robrechtc 10:22
onzekerheid, en dat een cell in meerdere niet-neighbouring milestones zou kunnen zitten
als in, biologisch gezien houdt het mss geen steek, maar een methode zou wel kunnen zeggen, "wete, ik denk dat het ofwel een DC is of een rode bloedcel"
* Wouter Saelend @wouters 10:24
hmmm uhu true, hoort allemaal wat samen met de onzekerheid. Je zou per cell een probabiliteit distributie doorheen het milestone netwerk kunnen geven, maar sla dat eens op 😛
* Robrecht.Cannoodt @robrechtc 10:25
uhu ^^
* Wouter Saelend @wouters 10:25
En bereken dan eens de emd 😛
* Robrecht.Cannoodt @robrechtc 10:25
idd...
deze discussie zou wel ergens een plaatsje kunnen krijgen in het manuscript, vind ik
* Wouter Saelend @wouters 10:25
Uhu in de discussie dan wss
* Robrecht.Cannoodt @robrechtc 10:26
tiens, een discussie die in de discussie zou plaats vinden, wat een goed idee 😮
<tease>
-->
# Methods


## Trajectory types


We classified all possible trajectory topologies into distinct trajectory types, based on topological criteria. These trajectory types start from the most general trajectory type, a disconnected simple graph, and moves down in a directed acyclic graph, progressively becoming more simple until the two basic types: linear and cycles (`r ref("sfig", "trajectory_type_tree", "a")`). For every type, a corresponding directed graph type can be defined by allowing directedness (`r ref("sfig", "trajectory_type_tree", "b")`).


```{r}
add_sfig(
figure_file("trajectory_type_trees.pdf","2-dataset_characterisation/2-trajectory_types"), 
"trajectory_type_tree", 
"Different trajectory types can be classified in a directed acyclic graph, in which certain types are a generalisation of other trajectory types.", 
15, 
10
)
```


## Trajectory inference methods


### Method wrapping




We gathered a list of `r sum(methods$is_ti, na.rm=TRUE)` trajectory inference methods (`r ref("table", "implementations", anchor=TRUE)`), by searching in literature for "trajectory inference" and "pseudotemporal ordering" and based on two existing lists found online [@davis_awesomesinglecelllistsoftware_2018; @gitter_singlecellpseudotimeoverviewalgorithms_2018]. We believe this to be the most complete list to data, but are welcome to include additional methods through e-mail or an issue on our github (https://github.com/dynverse/dynalysis). Methods were excluded from the evaluation based on several criteria: `r glue::collapse(glue::glue("({non_inclusion_reasons$footnote}) {non_inclusion_reasons$long}"), ", ")`. This resulted in the inclusion of `r sum(methods$evaluated)` methods in the evaluation (`r ref("table", "implementations")`).


Due to the absence of a common format for trajectory inference methods, most methods produce a heterogeneous output. We therefore created wrappers around each method in R (available at https://www.github.com/dynverse/dynmethods) and postprocessed its output into a common probabilistic trajectory model. This model consisted of two parts. The milestone network represents the overall network topology, and contains edges between different milestones and the length of the edge between them. The milestone percentages contain for each cell its position between two or more milestones, and sums for each cell to one.


Depending on the output of a method, we used different strategies to convert the output to our model (in some cases combined with a special conversion denoted by a \*):


```{r}
# create functions to list the methods within a particular conversion category
list_sentence <- function(x) {
  pritt(glue::collapse(x[1:length(x) - 1],", "), " and ", last(x))
}
conversion <- function(conversion_oi) {
 methods_evaluated %>% 
   filter(map_lgl(conversion_split, ~ conversion_oi %in% .)) %>% 
   mutate(text = paste0(method_name, ifelse(conversion_special, "\\*", ""))) %>%
   pull(text) %>% 
   list_sentence()
}
	

```


* Methods returning a single pseudotime value: `r conversion("linear")`. A single edge between two milestones, its percentage proportional to the pseudotime.
* Methods which assign each cell to a branch together with a pseudotime value and a branch network: `r conversion("branching_local")`. Similarly, the branch network is used as the milestone network, the percentages of a cell are proportional with its branch pseudotime.
* Methods returning both a global pseudotime value and a probability for every end state: `r conversion("branching_global")`. We use a single start state and add an edge to every end milestone each representing an end state. The global pseudotime then determines the distance from the begin milestone, the rest of the cell's position is calculated by distributing the residual percentage over the end states, proportionally to the end state probabilities.
* Methods returning a cluster assignment and a cluster network:  `r conversion("branching_cluster")`. The cluster network was used as milestone network, each cell received percentage 1 or 0 based on its cluster assignment.
* Methods returning a cluster assignment, cluster network and dimensionality reduction: `r conversion("branching_local_projection")`. We projected each cell on the closest point on the edges between its own cluster and neighbouring clusters within the dimensionality reduction. This projection allowed us to give each cell a pseudotime within the edge, which was then converted into our model as described above, using the cluster network as milestone network.


Special conversions were necessary for certain methods:


* DPT: We projected the cells onto the cluster network, consisting of a central milestone (this cluster contains the cells which were assigned to the "unknown" branch) and three terminal milestones, each corresponding to a tip point. This was then processed as described above.
* Sincell: To constrain the number of milestones this method creates, we merged two cell clusters iteratively until the percentage of leaf nodes was below a certain cutoff, default at 25%.  This was then processed as described above.
* SLICER: Although this method returns a branch network and a global pseudotime, it was not clear to us how to assign cells to individual branches in the network. We therefore used the cell graph provided by the method to calculate the shortest paths between the start and end cells found by the method, and then used this graph as the backbone for all other cells, creating a local pseudotime for every cell within its branch. The output was then further processed as above.
* SLICE: As discussed in the vignette of SLICE, we ran principal curves one by one for every edge detected by SLICE. This local pseudotime was then processed as above.
* MFA: We used the branch assignment as state probabilities, which together with the global pseudotime were processed as described above.


### Prior information


TODO: describe which prior information was given and how it was extracted
## Method quality control


```{r}
qc_references <- readRDS(derived_file("checks.rds", "4-method_characterisation"))$references %>% 
  str_split("[, \n]+") %>% 
  unlist() %>% 
  unique() %>% 
  discard(is.na) %>%
  glue::collapse(., ";") %>%
  paste0("[", ., "]")
```




We created a transparent scoring scheme to check the quality of each method (`r ref("stable", "qc_checks")`), based on several existing tool quality and programming guidelines in literature and online `r qc_references` . The goal of this quality control is in the first place to stimulate the improvement of current methods, and the development of user and developer friendly new methods. The quality control assessed 6 categories, each looking at several aspects, which are further divided into individual items. The availability category checks whether the method is easily available, whether the code and dependencies can be easily installed, and how the method can be used. The code quality assesses the quality of the code both from a user perspective (function naming, dummy proofing and availability of plotting functions) and a developer perspective (consistent style and code duplication). The code assurance category is frequently overlooked, and checks for code testing, continuous integration [@beaulieu-jones_reproducibility_2017] and an active support system. The documentation category checks the quality of the documentation, both externally (tutorials and function documentation) and internally (inline documentation). The behaviour category assesses the ease by which the method can be run, by looking for unexpected output files and messages, prior information and how easy the trajectory model can be extracted from the output. Finally, we also assessed the certain aspects of the study in which the method was proposed, such as publication in a peer-reviewed journal, the number of dataset on which the usefulness of the method was shown, and the scope of method evaluation in the paper.


For each quality aspect received a weight by how frequently it was found in several papers and online sources which discuss tool quality (`r ref("table", "implementations")`). This was to make sure that more important aspect, such as the open source availability of the method, outweigh other aspects, such as the availability of a graphical user interface. For calculating the final score, we weighed each of the six categories equally.


## Real datasets


We gathered a list of real datasets by searching for "single-cell" at the Gene Expression Omnibus and selecting those datasets in which the cells are sampled from different stages in a dynamic process (`r ref("stable", "datasets")`). The scripts to download and process these datasets are all available in our repository (http://www.github.com/dynverse/dynalysis). When available, we preferred to start from the raw counts data. These raw counts were all normalised and filtered using a common pipeline, discussed later. We determined a reference standard for every dataset using labelling provided by the author's, and classified the standards into gold and silver based on whether this labelling was determined by the expert using the expression (silver standard) or using other external information (such as FACS or the origin of the sample, gold standard) (`r ref("stable", "datasets")`).


## Synthetic datasets


Our workflow to generate synthetic data is based on the well established workflow used in the evaluation of network inference methods [@schaffter_genenetweaversilicobenchmark_2011; @marbach_wisdom_2012] and consists of four main steps: network generation, simulation, gold standard extraction and simulation of the scRNA-seq experiment. At every step, we took great care was taking to mimic real cellular regulatory networks as best as possible, while keeping the model simple and easily extendable. For every synthetic dataset, we used a random real dataset as a reference dataset (from those described earlier), making sure the number of variable genes and cells were similar.


### Network generation


One of the main process involved in cellular dynamic processes is gene regulation, where regulatory cascades and feedback loops lead to progressive changes in expression and decision making. The exact way a cell chooses a certain path during its differentiation is still an active research field, although certain models have already emerged and been tested in vivo. One driver of bifurcations seem to be mutual antagonism, where genes [@xu_regulationbifurcatingcell_2015] strongly repress each other, forcing one of the two to become inactive [@graf_forcingcellschange_2009]. Such mutual antagonism can be modelled and simulated [@wang_quantifyingwaddingtonlandscape_2011; @ferrell_bistabilitybifurcationswaddington_2012]. Although such a two-gene model is simple and elegant, the reality is frequently more complex, with multiple genes (grouped into models) repressing each other [@yosef_dynamicregulatorynetwork_2013].


<p class='wip'> Cascades? Cycle?</p>

<p class='wip'> To simulate certain trajectory topologies, we therefore designed module networks in which the cells will follow a particular trajectory topology given certain parameters (`r ref("sfig", "modulenets")`). Two module networks generated linear trajectories (linear and linear long), two generated simple forks (bifurcating and converging), one generated a complex fork (trifurcating), one generated a rooted tree (consecutive bifurcating) and two generated trajectories which simple undirected graphs (bifurcating loop and bifurcating convergence).</p>

From these module networks we generated gene regulatory networks in two steps: generation of the main regulatory network, and addition of extra target genes from real regulatory networks. For each dataset, we used the same number of genes as were differentially expressed in the real datasets. 5% of genes were assigned to be part of the main, and were randomly distributed among all modules (with at least one gene per module). We sampled edges between these individual genes (according to the module network) using an uniform distribution between 1 and the number of possible targets in each module. To add additional target genes to the network, we assigned every regulator from the network to a real regulator in a real network (from regulatory circuits [@marbach_tissuespecificregulatorycircuits_2016]), and extracted for every regulator a local network around it using personalized pagerank (with damping factor set to 0.1). 


```{r}
add_sfig({
read_rds(figure_file("modulenets.rds", experiment_id="2-dataset_characterisation/3-synthetic"))
}, 
"modulenets", 
"Module networks, milestone networks and the module dynamics for each of the different types of synthetic data", 
15, 
5
)
```


### Simulation of thermodynamics


To simulate the gene regulatory network, we used a system of differential equations similar to which has been standard in evaluations of network inference [@marbach_wisdom_2012]. In this model, changes in gene expression ($x_i$) and protein expression ($y_i$) are modeled using ordinary differential equations [@schaffter_genenetweaversilicobenchmark_2011] (ODEs):


$$
\begin{aligned}
\label{eq:mrna_ode}
\frac{dx_i}{dt} &= \underbrace{m \times f(y_1, y_2, ...)}_\text{production} - \underbrace{\lambda \times x_i}_\text{degradation}
\end{aligned}
$$
$$
\begin{aligned}
\label{eq:prot_ode}
\frac{dy_i}{dt} &= \underbrace{r \times x_i}_\text{production} - \underbrace{\Lambda \times y_i}_\text{degradation}
\end{aligned}
$$


where $m$, $\lambda$, $r$ and $\Lambda$ represent production and degradation rates, the ratio of which determines the maximal gene and protein expression. The two types of equations are coupled because the production of protein $y_i$ depends on the amount of gene expression $x_i$, which in turn depends on the amount of other proteins through the activation function $f(y_1, y_2, ...)$.


The activation function is inspired by a thermodynamic model of gene regulation, in which the promoter of a gene can be bound or unbound by a set of transcription factors, each representing a certain state of the promoter. Each state is linked with a relative activation $\alpha_j$, a number between 0 and 1 representing the activity of the promoter at this particular state. The production rate of the gene is calculated by combining the probabilities of the promoter being in each state with the relative activation:


$$
\begin{aligned}
\label{eq:input_funcion_probabilities}
f(y_1, y_2, ..., y_n) = \sum_{j \in \{0, 1, ..., n^2\}} \alpha_j \times P_j
\end{aligned}
$$


The probability of being in a state is based on the thermodynamics of transcription factor binding. When only one transcription factor is bound in a state:
$$
\begin{aligned}
P_j \propto \nu = \left(\frac{y}{k}\right)^{n}
\end{aligned}
$$


Where the hill coefficient $n$ represents the cooperativity of binding and $k$ the transcription factor concentration at half-maximal binding. When multiple regulators are bound:
$$
\begin{aligned}
P_j \propto \nu =  \rho \times \prod_j \left(\frac{y_j}{k_j}\right)^{n_j}
\end{aligned}
$$


where $\rho$ represents the cooperativity of binding between the different transcription factors. 


$P_i$ is only proportional to $\nu$ because $\nu$ is normalized such that $\sum_{i} P_i = 1$.


To each differential equation, we added an additional stochastic term: 
$$
\begin{aligned}
\frac{dx_i}{dt} = m \times f(y_1, y_2, ...) - \lambda \times x_i &+ \eta \times \sqrt{x_i} \times \Delta W_t \\
\frac{dy_i}{dt} = r \times x_i - \Lambda \times y_i &+ \eta \times \sqrt{y_i} \times \Delta W_t
\end{aligned}
$$


with $\Delta W_t \sim \mathcal{N}(0, h)$. 


Similar to [@schaffter_genenetweaversilicobenchmark_2011], we sample the different parameters from random distributions, given in `r ref("stable", "samplers")`.


```{r}
add_stable(
read_rds(figure_file("samplers.rds","2-dataset_characterisation/3-synthetic"))[[params$table_format]], 
"samplers", 
"Distributions from which each parameter in the thermodynamic model was sampled"
)
```


These SDEs were simulated using the Euler–Maruyama approximation, with time-step $h = 0.01$ and noise strength $\eta = 8$. The total simulation time varied between 5 for linear and bifurcating datasets, 10 for consecutive bifurcating, trifurcating and converging datasets, 15 for bifurcating converging datasets and 30 for linear long, cycle and bifurcating loop datasets. The burn-in period was for each simulation 2. Each network was simulated 32 times.


### Simulation of the single-cell RNA-seq experiment


For each dataset we sampled the same number of cells as were present in the reference real dataset, limited to the simulation steps after burn-in. Next, we used the Splatter package [@zappia_splattersimulationsinglecell_2017] to estimate the different characteristics of a real dataset, such as the distributions of average gene expression, library sizes and dropout probabilities. We used Splatter to simulate the expression levels $\lambda_{i,j}$ of housekeeping genes $i$ (to match the number of genes in the reference dataset) in every cell $j$. These were combined with the expression levels of the genes simulated within a trajectory. Next, true counts were simulated using $Y'_{i,j} \sim \text{Poi}(\lambda_{i,j})$. Finally, we simulated dropouts by setting true counts to zero by sampling from a Bernouilli distribution using a dropout probability $\pi^D_{i,j} =\frac{1}{1+e^{-k(\text{ln}(\lambda_{i,j})-x_0)}}$.


### Gold standard extraction


<p class='wip'> Because each cellular simulations follows the trajectory it its own speed, knowing the exact position of a cell within the trajectory toplogy is not straightforward. Furthermore, the speed by which simulated cells made a decision between two or more alternative paths was highly variable sfig?. To estimate a cell's position during a simulation within the trajectory toplogy, we therefore used the known progression of the modules, given in `r ref("sfig", "modulenets", "c")`,  as a backbone. We smoothed the expression in each simulation using a rolling mean with a window of 50 time steps, and then calculated the average module expression along the simulation. We used dynamic time warping, implemented in the dtw R package [@giorgino_computingvisualizingdynamic_2009; @tormene_matchingincompletetime_2009], with an open end to align a simulation to all possible module progressions, and then picked the alignment which minimised the normalised distance between the simulation and the backbone. In case of cyclical trajectory topologies, the number of possible milestones a backbone could progress through was limited to 20.</p>

## Common normalisation pipeline
We used a standard single-cell RNA-seq preprocessing pipeline which applies parts of the scran and scater Bioconductor packages [@lun_stepbystepworkflowlowlevel_2016]. The advantages of this pipeline is that it works both with and without spike-ins, and includes a harsh cell filtering which looks at abnormalities in library sizes, mitochondrial gene expression, and number of genes expressed using median absolute deviations (set to 3). We required that a gene has to be expressed in at least 5% of the cells, and that it should have an average expression higher than 0.05. Furthermore, we used the pipeline to select the most highly variable genes, using a false discovery rate of 5% and a biological component higher than 0.5. As a final filter, we removed both all zero genes and cells until convergence.


## Evaluation metrics
We used three different metrics to assess the performance of each TI method. The importance of using multiple metrics to compare complex models has been stated repeatedly REFS. Indeed, each of our metrics looks at the correspondence between prediction and gold standard in a different way: the correlation between geodesic distances assesses the overall ordering, the RF prediction error assesses the cell neighbourhood, and the edge flip score assesses the milestone network topology (`r ref("sfig", "score_aspects")`).


```{r}
add_sfig({
read_rds(figure_file("score_aspects.rds", experiment_id="3-score_rules/5-score_aspects"))
}, 
"score_aspects", 
"Different scores assess different aspects of the ", 
15, 
8
)
```


### Correlation between geodesic distances
The accuracy of cell ordering was assessed by first calculating the geodesic distances $d(c_i, c_j)$ between all cells in both the gold standard and the prediction. This geodesic distance is calculated by finding the shortest path between two cells through the milestone network, while taking into account the edge lengths as returned by most methods. To calculate this for every pair of cells, we first calculated the shortest path lengths $d(m_a, m_b)$ between all milestones using Dijkstra's algorithm, as implemented in the R *igraph* package. We also calculated $d(m_a, c_i)$ for every cell to its neighbouring milestones by calculating the earth's movers distance using the shortlist method [@gottschlich_shortlistmethodfast_2014], as implemented in the R *transport* package. Then we again used Dijkstra's algorithm to calculate the shortest path lengths between every pair of cells which minimises:


$$
d(c_i, c_j) = min_{a \in A, b \in B} d(m_a, c_i) + d(m_a, m_b) + d(m_b, c_j)
$$


where A and B contain all milestones adjacent to respectively $c_i$ and $c_j$.
The final score was then calculated the Spearman's rank correlation between all $d(c_i, c_j)$ in gold standard and prediction.


In some cases, methods are unable to place some cells on a trajectory, mainly because they classify these cells as noisy. In these cases, we add these cells to a new milestone and put this milestone at a distance of 5 times the maximal distance between two milestones within the network.


### Random forest prediction error


Although the correlation between geodesic distances directly assesses the position of the cells in the trajectory, a bad correlation does not directly imply that similar cells were not grouped together by the method, as illustrated in `r ref("sfig", "score_aspects", c("b", "c"))`. Indeed, certain methods will inevitably reach an incorrect ordering because they cannot handle the correct trajectory type, but these methods could still correctly place similar cells next to each other. We therefore also included a metric which looks at the local neighbourhood of each cell, and assesses whether this neighbourhood can accurately predict the position of this cell in the gold standard. We used a Random Forest regression, implemented in the ranger package [@wright_rangerfastimplementation_2017] to separately predict milestone percentages of every cell in the gold standard, using the milestone percentages of these cells in the prediction as features. We then used the out-of-bag mean-squared error on these percentages to score each method's capability of predicting the correct neighbourhood of each cell.


### Edge flip score


As a third independent score, we assessed the similarity between the milestone network topology. We first simplified each network, by merging consecutive linear edges into one edge, by adding new milestones within self loops such that A->A would be converted A->B->C->D, by adding an intermediate node to linear networks. Because we are interested , the network was made undirected. Next, we define the edge flip score as the minimal number of edges which should be added or removed to convert one network into the other, divided by the total number of edges in both networks. This problem is equivalent to the maximum common edge subgraph problem, a known NP-hard problem with no suitable [@bahiense_maximumcommonedge_2012]. We implemented a branch & bound approach for this problem, by first enumerating all possible edge additions and removals with the minimal number of edges (the edge difference between the two networks) and if none of the new networks was isomorphic, try out all solutions with an additional two edge changes. To further limit the search space, we make sure the degree distributions between the two networks are similar, before assessing whether the two networks are isomorphic using the BLISS algorithm [@junttila_engineeringefficientcanonical_2007], as implemented in the R *igraph* package.


```{r}
add_sfig({
figure_file("edge_flip_comparison.pdf", experiment_id="3-score_rules/4-edge_flip")
}, 
"edge_flip_comparison", 
"Edge flip score values for common trajectory topologies", 
15, 
5
)
```


# Acknowledgements


# Supplementary Material


```{r "supplementary", echo=FALSE, results='asis'}
source("analysis/paper/sfigs_stables.R")
```


# Colophon


This report was generated on `r Sys.time()` using `r devtools:::platform_info()$version` and the following packages:
```{r colophon, cache = FALSE}
devtools::session_info()$packages %>% knitr::kable()
```


The current Git commit details are:


```{r}
git2r::repository(".")
```
I think we should mention something here about sc transcriptomics, flow/mass cytometry etc...
For example, I talked with Abhishek the other day and his view was really flow oriented. I think people who primarily use flow will be confused by whether this paper looks at scRNAseq or both flow and scRNAseq
Love this figure! It's a great summary of the /whole/ study methodology
I have several comments (that I might address myself if I get to it first). I'll post them as separate comments, so they can get resolved individually.
I would include a part for the method wrappers; that is the way that linear, cellcluster, cellgraph and generic TI's get translated to the probabilistic trajectory model
part c: I would explicitly show that the probabilistic trajectory models are being used for both the tasks and for the method outputs
The spacing is a bit variable; e.g. part b takes up a lot of real estate while parts d and e seem to be packed very tightly together
part f: I'd explicitly mention that each column is to be interpreted separately; there is no connection horizontally. JUST TO MAKE SURE.
a handful of the most recent methods
Repetition of most: put "The majority of" instead?
..