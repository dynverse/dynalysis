---
title: "A comparison of single-cell trajectory inference methods: towards more accurate and robust tools"
bibliography: references.bib
csl: nature-biotechnology.csl
---


*`r format(Sys.time(), '%d %B, %Y')`*
<!--
Tips for working in this document:
* Go to tools -> perferences and remove Use smart quotes
* Add extra enters after every paragraph (this is markdown!)


TITLES: 
Not all trajectory inference methods are created equal
Trajectory inference: work in progress
Trajectory inference: we’re not quite there yet
Paving the way for trajectories
Benchmarking trajectory inference methods
Bringing single-cell trajectory inference methods to the next level
Resolving disarray amongst trajectory inference methods with a comprehensive evaluation
A comparison of trajectory inference methods
Trajectory inference 2.0
We’re on the trajectory to better trajectory inference methods
On the road to more accurate and robust single-cell trajectory inference methods
Towards more accurate and robust single-cell trajectory inference methods
On the trajectory to more accurate,robust and trajectory-like single-cell trajectory inference method trajectories
A comparison of single-cell trajectory inference methods: towards more accurate and robust tools
-->


```{r setup, include=FALSE}
source("setup.R")
```
`r glue::collapse(sample(c("Wouter Saelens* ¹ ²", "Robrecht Cannoodt* ¹ ² ³")), ", ")`, Helena Todorov¹ ² ⁴ ⁵, Yvan Saeys¹ ²


\* Equal contribution  
¹ Data mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium.  
² Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium.  
³ Center for Medical Genetics, Ghent University Hospital, Ghent, Belgium.  
⁴ CIRI, Centre International de Recherche en Infectiologie, Inserm, Lyon, France.  
⁵ Université Claude Bernard Lyon 1, Lyon, France.  


# Abstract


Using single-cell transcriptomics data, it is now possible to computationally order cells along trajectories, allowing the unbiased transcriptome-wide study of cellular dynamic processes. Since 2014, more than 50 trajectory inference methods have been developed, each with its own set of methodological characteristics. This makes a direct comparison between trajectory inference methods often difficult, and as a result a comprehensive assessment of the performance and robustness of each method is still lacking. We developed a framework of data topologies for trajectory inference methods in order to make their results comparable to each other and to a gold standard, when available. In this work, we compare the results from a total of 22 trajectory inference methods, on a large collection of real datasets and realistic synthetic datasets. We employ a novel parameter optimisation procedure, and We compare methods using several metrics, including accuracy of the inferred ordering, correctness of the network wiring, code quality and user friendliness. We conclude with practical guidelines for method users, and also believe our evaluation framework can be used to spearhead the development for new methods. Our evaluation pipeline is fully reproducible, can be easily extended, and is available at https://www.github.com/dynverse.


# Introduction


Single-cell -omics technologies now make it possible to more accurately model biological systems than ever before. One area where single-cell data can be particularly useful is in the study of cellular dynamic processes, such as the cell cycle, cell differentiation and cell activation. When cells are sampled from a population in which cells are at different unknown points in the dynamic process, trajectory inference methods can be used to computationally order these cells along their dynamic process [@cannoodt_computational_2016]. Because they offer an unbiased and transcriptome-wide understanding of the dynamic process, trajectories then allow the identification of new (primed) subsets of cells, delineation the exact wiring of a differentiation tree and inference of regulatory interaction responsible for a bifurcation refs.


```{r}
add_sfig({
n_methods_over_time <- readRDS(figure_file("n_methods_over_time.rds", experiment_id="method_characteristics"))
trajectory_components_over_time <- readRDS(figure_file("trajectory_components_over_time.rds", experiment_id="method_characteristics"))


cowplot::plot_grid(plotlist=list(n_methods_over_time, trajectory_components_over_time), nrow=2, rel_heights = c(0.7, 0.3), labels="auto")
}, 
"methods_over_time", 
"New TI methods over time. **a** Number of methods published or in preprint. **b** Number of methods which can handle a particular type of trajectory.
", 
15, 
10
)
```


<!-- history of Ti, different methods -->
Dozens of trajectory inference methods have been developed over the last years, and more are being published almost every month (`r ref("sfig", "methods_over_time", "a")`). Initially, most trajectory inference fixed the trajectory topology to a linear or bifurcating topology, by either algorithmic limitations or through user parameters, and focussed on correctly ordering the cells along this fixed topology. However, several recent methods now infer both the ordering and the topology at the same time, although certain structural limitations are still imposed(`r ref("sfig", "methods_over_time", "b")`). Methods which do not impose limitations on the topology are now being developed [@welch_slicer:_2016; @wolf_graph_2017], and it is expected that in the future methods will be able to model even more complex behavior, such as multiple dynamic processes happening at parallel in a single-cell or the integration of datasets from different patients [@tanay_scaling_2017; @cannoodt_computational_2016].


<!-- Common framework of TI, different components, naturally progressing towards the need for an evaluation -->
Although trajectory inference methods use a variety of algorithms and subcomponents to reach a final ordering, most consist of the following three steps: (i) preprocessing (normalization, filtering of genes and/or cells), (ii) conversion to a simplified representation using dimensionality reduction, clustering or graph building and (iii) ordering the cells along the simplified representation [@cannoodt_computational_2016]. Depending on the way the cells are ordered, some prior information about the dynamic process can be optionally used or required by the method, which can both bias the trajectory to current knowledge but also guide the method towards choosing the right trajectory among many others. Furthermore, with the ongoing strong scalability in single-cell -omics technologies, it becomes more and more important that analysis methods become scalable and user friendly .


<!-- Common framework of TI, different components, naturally progressing towards the need for an evaluation -->
Given this plethora of available trajectory inference methods, it is important that methods are compared, so that users can use the most optimal method available for their problem. Moreover, new methods need to be rigorously tested, so that development can focus on improving the current state-of-the-art. In this study, we therefore for the first time developed a comprehensive evaluation framework for trajectory inference methods. We test both on synthetic data, for which the gold standard is known, and real data, for which in some cases the gold standard can be extracted from expert knowledge. We include a rigorous parameter optimisation, making sure that parameters are optimized while avoiding overfitting on characteristics of specific datasets. To make our framework reusable and extendable, we make use of continuous analysis [@beaulieu-jones_reproducibility_2017], which allows the developers of new methods to easily test their methods and compare them against the state-of-the-art.


# Results
## Method quality control


```{r}
# Load method characteristics
methods <- readRDS(derived_file("methods.rds", experiment_id="method_characteristics"))
```


We gathered a list of `r sum(methods$is_ti, na.rm=TRUE)` TI methods from literature, and selected a subset of `r sum(methods$evaluated)` for evaluation, primarily based on their free availability and the presence of a programming interface `r ref("table", "methods")`. We characterized all methods in four different ways: possible trajectory topologies, implementation quality, prior information and the underlying algorithm (`r ref("fig", "method_characteristics")`).


`r ref("table", "methods", anchor=TRUE)` Overview of current trajectory inference methods, and whether they were included in this current evaluation study
```{r}
read_rds(figure_file("methods_table.rds", "method_characteristics"))
```


At the level of trajectory topologies, most recent methods can handle one or more splits (either bifurcations or multifurcations) in the trajectory (`r ref("fig", "method_characteristics", "a")`). However, only a handful of more recent methods can currently handle more complex graph topologies which can include loops (`r ref("sfig", "methods_over_time", "b")`), one of which is included in the current evaluation (SLICER).


While not directly related to the accuracy of the inferred trajectory, the quality of the implementation is an important first evaluation metric, because good unit testing makes sure the implementation is correct, good documentation makes it easier for potential users to apply the method on their data, and overall good code quality makes it possible for other developers to adapt the method and extend it further. We therefore looked at the implementation of each method, and assessed their quality using a transparent scoring scheme (`r ref("stable", "qc_checks")`). The individual quality checks can be grouped in two ways: what aspect of the method they investigate (availability, code quality, code assurance, documentation, behaviour and the paper) or for which purpose(s) they are important (user friendliness, developer friendliness or good science). These categorisations can then be used to help current developers to improve their tool, and to guide the selection of users and developers to use these tools for their purpose. After publishing this preprint, we will contact the authors of each method, allowing them to improve their method before the final publishing of the evaluation.


```{r}
add_stable(
read_rds(figure_file("qc_checks.rds","method_characteristics")), 
"qc_checks", 
""
)
```


Most methods fulfilling most of the basic criteria, such as free availability and basic code quality criteria (`r ref("fig", "method_characteristics", "b")`). Recent methods tend to have higher code quality (`r ref("sfig", "qc_over_time")`). However, several aspects are consistently lacking for the majority of the methods (`r ref("fig", "method_characteristics", "e")`) and we believe that these should receive extra attention from developers. Although these outstanding issues cover all five categories, code assurance and documentation in particular are problematic areas (`r ref("sfig", "method_qc_check")` and `r ref("fig", "method_characteristics", "c")`), notwithstanding studies pinpointing these as good practices [@wilson_best_2014].


The underlying algorithms can be divided into several modules, and these frequently contain soms clustering, dimensionality reduction and/or graph building steps. Interestingly, modules are frequently shared between different algorithms (`r ref("sfig", "method_components")`).


`r anchor("fig", "method_characteristics")`
![](`r figure_file("method_characteristics.svg", experiment_id="method_characteristics")`)
`r ref("fig", "method_characteristics", anchor=TRUE)` First characterization of each method.


```{r}
add_sfig({
readRDS(figure_file("method_qc_check.rds", experiment_id="method_characteristics"))
}, 
"method_qc_check", 
"Caption", 
10, 
13
)
```


```{r}
add_sfig({
readRDS(figure_file("method_components.rds", experiment_id="method_characteristics"))
}, 
"method_components", 
"TI methods frequently share steps", 
15, 
7.5
)
```




```{r}
add_sfig({
readRDS(figure_file("qc_over_time.rds", experiment_id="method_characteristics"))
}, 
"qc_over_time", 
"Quality control scores and number of citations over time", 
10, 
10
)
```
## Evaluation overview


`r anchor("fig", "evaluation_overview")`
![](`r figure_file("ti_evaluation_overview_v1.svg", experiment_id="evaluation_overview")`)
`r ref("fig", "evaluation_overview")` Overview of the evaluation


```{r}
add_stable(
read_rds(figure_file("datasets.rds","dataset_characterisation")), 
"datasets", 
"Real datasets used in this study"
)
```






# Discussion


# Methods


## Trajectory types


To classify the output of methods and the gold standards, we defined a set of basic trajectory types. These trajectory types start from the most general trajectory type, a disconnected simple graph, and moves down in a directed acyclic graph using certain graph properties, progressively becoming more simple until the two basic types: linear and cycles `r ref("sfig", "trajectory_type_tree", "a")`. For every type, a corresponding directed graph type can be defined by allowing directedness `r ref("sfig", "trajectory_type_tree", "b")`.


```{r}
add_sfig({
readRDS(figure_file("trajectory_type_trees.rds", experiment_id="dataset_characterisation/trajectory_types"))
}, 
"trajectory_type_tree", 
"Different trajectory types can be classified in a directed acyclic graph, in which certain types are a generalisation of other trajectory types.", 
15, 
10
)
```


## Trajectory inference methods


We gathered a list of `r sum(methods$is_ti, na.rm=TRUE)` trajectory inference methods `r ref("table", "methods", anchor=TRUE)`, based on two existing lists found online [@davis_awesomesinglecelllistsoftware_2018; @gitter_singlecellpseudotimeoverviewalgorithms_2018] and by searching in literature for "trajectory inference" and "pseudotemporal ordering". We believe this to be the most complete list to data, but are welcome to include additional methods through e-mail or an issue on our github (https://github.com/dynverse/dynalysis). Methods were excluded from the evaluation based on several criteria: `r glue::collapse(glue::glue("({non_inclusion_reasons$footnote}) {non_inclusion_reasons$long}"), ", ")`. This resulted in the inclusion of `r sum(methods$evaluated)` methods in the evaluation (`r ref("table", "methods")`).


<p class='wip'> We wrapped each method (available at https://www.github.com/dynverse/dynmethods) and postprocessed its output into a common probabilistic trajectory model. This model consists of two parts. The milestone network represents the overall network topology, and contains of edges between different milestones and the length of the edge between them. The milestone percentages contain for each cell its position between two or more milestones, and sums for each cell to one. Linear methods are the easiest to wrap into this format, with two milestones and all cells in between. Methods such as </p>## Method quality control


We created a transparent scoring scheme to check the quality of each method (`r ref("table", "methods")`). The quality control assessed 6 categories, each looking at several aspects, which are further divided into individual items. The goal of this quality control is in the first place to stimulate the improvement of current methods, and the development of user and developer friendly new methods. The availability category checks whether the method is easily available, whether the code and dependencies can be easily installed, and how the method can be used. The code quality assesses the quality of the code both from a user perspective (function naming, dummy proofing and availability of plotting functions) and a developer perspective (consistent style and code duplication). The code assurance category is frequently overlooked, and checks for code testing, continuous integration [@beaulieu-jones_reproducibility_2017] and an active support system. The documentation category checks the quality of the documentation, both externally (tutorials and function documentation) and internally (inline documentation). The behaviour category assesses the ease by which the method can be run, by looking for unexpected output files and messages, prior information and how easy the trajectory model can be extracted from the output. Finally, we also assessed the certain aspects of the study in which the method was proposed, such as publication in a peer-reviewed journal, the number of dataset on which the usefulness of the method was shown, and the scope of method evaluation in the paper.


For each quality aspect received a weight by how frequently it was found in several papers and online sources which discuss tool quality (`r ref("table", "methods")`). This was to prevent make sure more important aspect, such as the open source availability of the method, to outweigh other aspects, such as the availability of a graphical user interface. For calculating the final score, we however weighed each of the six categories equally.


## Real datasets


We gathered a list of real datasets by searching for "single-cell" at the Gene Expression Omnibus and selecting those datasets in which the cells are sampled from different stages in a dynamic process (`r ref("stable", "datasets")`). The scripts to download and process these datasets are all available in our repository (http://www.github.com/dynverse/dynalysis). When available, we preferred to start from the raw counts data. These raw counts were all normalised and filtered using a common pipeline, discussed later. We determined a reference standard for every dataset using labelling provided by the author's, and classified the standards into gold and silver based on whether this labelling was determined by the expert using the expression (silver standard) or using other external information (such as FACS or the origin of the sample, gold standard) (`r ref("stable", "datasets")`).


## Synthetic datasets


Our workflow to generate synthetic data is based on the well established workflow used in the evaluation of network inference methods [@schaffter_genenetweaversilicobenchmark_2011; @marbach_wisdom_2012] and consists of four main steps: network generation, simulation, gold standard extraction and simulation of the scRNA-seq experiment. At every step, we took great care was taking to mimic real cellular regulatory networks as best as possible, while keeping the model simple and easily extendable. For every synthetic dataset, we used a random real dataset as a reference dataset (from those described earlier), making sure the number of variable genes and cells were similar.


### Network generation


One of the main process involved in cellular dynamic processes is through gene regulation, where regulatory cascades and feedback loops lead to progressive changes in expression and decision making. The exact way a cell chooses a certain path during its differentiation is still an active research field, although certain models have already emerged and been tested in vivo. One driver of bifurcations seem to be mutual antagonism, where genes [@xu_regulationbifurcatingcell_2015] strongly repress each other, forcing one of the two to become inactive [@graf_forcingcellschange_2009]. Such mutual antagonism can be modelled and simulated [@wang_quantifyingwaddingtonlandscape_2011; @ferrell_bistabilitybifurcationswaddington_2012]. Although such a two-gene model is simple and elegant, the reality is frequently more complex, with multiple genes (grouped into models) repressing each other [@yosef_dynamicregulatorynetwork_2013].


<p class='wip'> Cascades? Cycle?</p>

<p class='wip'> To simulate certain trajectory topologies, we therefore designed module networks in which the cells will follow a particular trajectory topology given certain parameters (`r ref("sfig", "modulenets")`). Two module networks generated linear trajectories (linear and linear long), two generated simple forks (bifurcating and converging), one generated a complex fork (trifurcating), one generated a rooted tree (consecutive bifurcating) and two generated trajectories which simple undirected graphs (bifurcating loop and bifurcating convergence).</p>

From these module networks we generated gene regulatory networks in two steps: generation of the main regulatory network, and addition of extra target genes from real regulatory networks. For each dataset, we used the same number of genes as were differentially expressed in the real datasets. 5% of genes were assigned to be part of the main, and were randomly distributed among all modules (with at least one gene per module). We sampled edges between these individual genes (according to the module network) using an uniform distribution between 1 and the number of possible targets in each module. To add additional target genes to the network, we assigned every regulator from the network to a real regulator in a real network (from regulatory circuits [@marbach_tissuespecificregulatorycircuits_2016]), and extracted for every regulator a local network around it using personalized pagerank (with damping factor set to 0.1). 


```{r}
add_sfig({
readRDS(figure_file("modulenets.rds", experiment_id="dataset_characterisation/synthetic"))
}, 
"modulenets", 
"Module networks, milestone networks and the module dynamics for each of the different types of synthetic data", 
15, 
5
)
```


### Simulation of thermodynamics


To simulate the gene regulatory network, we used a system of differential equations similar to which has been standard in evaluations of network inference [@marbach_wisdom_2012]. In this model, changes in gene expression ($x_i$) and protein expression ($y_i$) are modeled using ordinary differential equations [@schaffter_genenetweaversilicobenchmark_2011] (ODEs):


$$
\begin{align}
\label{eq:mrna_ode}
\frac{dx_i}{dt} &= \underbrace{m \times f(y_1, y_2, ...)}_\text{production} - \underbrace{\lambda \times x_i}_\text{degradation} \\
\label{eq:prot_ode}
\frac{dy_i}{dt} &= \underbrace{r \times x_i}_\text{production} - \underbrace{\Lambda \times y_i}_\text{degradation}
\end{align}
$$


where $m$, $\lambda$, $r$ and $\Lambda$ represent production and degradation rates, the ratio of which determines the maximal gene and protein expression. The two types of equations are coupled because the production of protein $y_i$ depends on the amount of gene expression $x_i$, which in turn depends on the amount of other proteins through the activation function $f(y_1, y_2, ...)$.


The activation function is inspired by a thermodynamic model of gene regulation, in which the promoter of a gene can be bound or unbound by a set of transcription factors, each representing a certain state of the promoter. Each state is linked with a relative activation $\alpha_j$, a number between 0 and 1 representing the activity of the promoter at this particular state. The production rate of the gene is calculated by combining the probabilities of the promoter being in each state with the relative activation:


$$
\begin{align}
\label{eq:input_funcion_probabilities}
f(y_1, y_2, ..., y_n) = \sum_{j \in \{0, 1, ..., n^2\}} \alpha_j \times P_j
\end{align}
$$


The probability of being in a state is based on the thermodynamics of transcription factor binding. When only one transcription factor is bound in a state:
$$
\begin{align}
P_j \propto \nu = \left(\frac{y}{k}\right)^{n}
\end{align}
$$


Where the hill coefficient $n$ represents the cooperativity of binding and $k$ the transcription factor concentration at half-maximal binding. When multiple regulators are bound:
$$
\begin{align}
P_j \propto \nu =  \rho \times \prod_j \left(\frac{y_j}{k_j}\right)^{n_j}
\end{align}
$$


where $\rho$ represents the cooperativity of binding between the different transcription factors. 


$P_i$ is only proportional to $\nu$ because $\nu$ is normalized such that $\sum_{i} P_i = 1$.


To each differential equation, we added an additional stochastic term: 
$$
\begin{align}
\frac{dx_i}{dt} = m \times f(y_1, y_2, ...) - \lambda \times x_i &+ \eta \times \sqrt{x_i} \times \Delta W_t \\
\frac{dy_i}{dt} = r \times x_i - \Lambda \times y_i &+ \eta \times \sqrt{y_i} \times \Delta W_t
\end{align}
$$


with $\Delta W_t \sim \mathcal{N}(0, h)$. 


Similar to [@schaffter_genenetweaversilicobenchmark_2011], we sample the different parameters from random distributions, given in `r ref("stable", "samplers")`.


```{r}
add_stable(
read_rds(figure_file("samplers.rds","dataset_characterisation/synthetic")), 
"samplers", 
"Distributions from which each parameter in the thermodynamic model was sampled"
)
```


These SDEs were simulated using the Euler–Maruyama approximation, with time-step $h = 0.01$ and noise strength $\eta = 8$. The total simulation time varied between 5 for linear and bifurcating datasets, 10 for consecutive bifurcating, trifurcating and converging datasets, 15 for bifurcating converging datasets and 30 for linear long, cycle and bifurcating loop datasets. The burn-in period was for each simulation 2. Each network was simulated 32 times.


### Simulation of the single-cell RNA-seq experiment


For each dataset we sampled the same number of cells as were present in the reference real dataset, limited to the simulation steps after burn-in. Next, we used the Splatter package [@zappia_splattersimulationsinglecell_2017] to estimate the different characteristics of a real dataset, such as the distributions of average gene expression, library sizes and dropout probabilities. We used Splatter to simulate the expression levels $\lambda_{i,j}$ of housekeeping genes $i$ (to match the number of genes in the reference dataset) in every cell $j$. These were combined with the expression levels of the genes simulated within a trajectory. Next, true counts were simulated using $Y'_{i,j} \sim \text{Poi}(\lambda_{i,j})$. Finally, we simulated dropouts by setting true counts to zero by sampling from a Bernouilli distribution using a dropout probability $\pi^D_{i,j} =\frac{1}{1+e^{-k(\text{ln}(\lambda_{i,j})-x_0)}}$.


### Gold standard extraction


<p class='wip'> Because each cellular simulations follows the trajectory it its own speed, knowing the exact position of a cell within the trajectory toplogy is not straightforward. Furthermore, the speed by which simulated cells made a decision between two or more alternative paths was highly variable sfig?. To estimate a cell's position during a simulation within the trajectory toplogy, we therefore used the known progression of the modules, given in `r ref("sfig", "modulenets", "c")`,  as a backbone. We smoothed the expression in each simulation using a rolling mean with a window of 50 time steps, and then calculated the average module expression along the simulation. We used dynamic time warping, implemented in the dtw R package [@giorgino_computingvisualizingdynamic_2009; @tormene_matchingincompletetime_2009], with an open end to align a simulation to all possible module progressions, and then picked the alignment which minimised the normalised distance between the simulation and the backbone. In case of cyclical trajectory topologies, the number of possible milestones a backbone could progress through was limited to 20.</p>

## Common normalisation pipeline
We used a standard single-cell RNA-seq preprocessing pipeline which applies parts of the scran and scater Bioconductor packages [@lun_stepbystepworkflowlowlevel_2016]. The advantages of this pipeline is that it works both with and without spike-ins, and includes a harsh cell filtering which looks at abnormalities in library sizes, mitochondrial gene expression, and number of genes expressed using median absolute deviations (set to 3). We required that a gene has to be expressed in at least 5% of the cells, and that it should have an average expression higher than 0.05. Furthermore, we used the pipeline to select the most highly variable genes, using a false discovery rate of 5% and a biological component higher than 0.5. As a final filter, we removed both all zero genes and cells until convergence.


## Evaluation metrics
We used three different metrics to assess the performance of each TI method. The importance of using multiple metrics to compare complex models has been stated repeatedly REFS. Indeed, each of our metrics looks at the correspondence between prediction and gold standard in a different way: the correlation between geodesic distances assesses the overall ordering, the RF prediction error assesses the cell neighbourhood, and the edge flip score assesses the milestone network topology (`r ref("sfig", "score_aspects")`).


```{r}
add_sfig({
readRDS(figure_file("score_aspects.rds", experiment_id="score_rules/score_aspects"))
}, 
"score_aspects", 
"Different scores assess different aspects of the ", 
15, 
5
)
```


### Correlation between geodesic distances
The accuracy of cell ordering was assessed by first calculating the geodesic distances $d(c_i, c_j)$ between all cells in both the gold standard and the prediction. This geodesic distance is calculated by finding the shortest path between two cells through the milestone network, while taking into account the edge lengths as returned by most methods. To calculate this for every pair of cells, we first calculated the shortest path lengths $d(m_a, m_b)$ between all milestones using Dijkstra's algorithm, as implemented in the R *igraph* package. We also calculated $d(m_a, c_i)$ for every cell to its neighbouring milestones by calculating the earth's movers distance using the shortlist method [@gottschlich_shortlistmethodfast_2014], as implemented in the R *transport* package. Then we again used Dijkstra's algorithm to calculate the shortest path lengths between every pair of cells which minimises:


$$
d(c_i, c_j) = min_{a \in A, b \in B} d(m_a, c_i) + d(m_a, m_b) + d(m_b, c_j)
$$


where A and B contain all milestones adjacent to respectively $c_i$ and $c_j$.
The final score was then calculated the Spearman's rank correlation between all $d(c_i, c_j)$ in gold standard and prediction.


In some cases, methods are unable to place some cells on a trajectory, mainly because they classify these cells as noisy. In these cases, we add these cells to a new milestone and put this milestone at a distance of 5 times the maximal distance between two milestones within the network.


### Random forest prediction error


Although the correlation between geodesic distances directly assesses the position of the cells in the trajectory, a bad correlation does not directly imply that similar cells were not grouped together by the method, as illustrated in `r ref("sfig", "score_aspects", c("b", "c"))`. Indeed, certain methods will inevitably reach an incorrect ordering because they cannot handle the correct trajectory type, but these methods could still correctly place similar cells next to each other. We therefore also included a metric which looks at the local neighbourhood of each cell, and assesses whether this neighbourhood can accurately predict the position of this cell in the gold standard. We used a Random Forest regression, implemented in the ranger package [@wright_rangerfastimplementation_2017] to separately predict milestone percentages of every cell in the gold standard, using the milestone percentages of these cells in the prediction as features. We then used the out-of-bag mean-squared error on these percentages to score each method's capability of predicting the correct neighbourhood of each cell.


### Edge flip score


As a third independent score, we assessed the similarity between the milestone network topology. We first simplified each network, by merging consecutive linear edges into one edge, by adding new milestones within self loops such that A->A would be converted A->B->C->D, by adding an intermediate node to linear networks. Because we are interested , the network was made undirected. Next, we define the edge flip score as the minimal number of edges which should be added or removed to convert one network into the other, divided by the total number of edges in both networks. This problem is equivalent to the maximum common edge subgraph problem, a known NP-hard problem with no suitable [@bahiense_maximumcommonedge_2012]. We implemented a branch&bound approach for this problem, by first enumerating all possible edge additions and removals with the minimal number of edges (the edge difference between the two networks) and if none of the new networks was isomorphic, try out all solutions with an additional two edge changes. To further limit the search space, we make sure the degree distributions between the two networks are similar, before assessing whether the two networks are isomorphic using the BLISS algorithm [@junttila_engineeringefficientcanonical_2007], as implemented in the R *igraph* package.


```{r}
add_sfig({
readRDS(figure_file("edge_flip_comparison.rds", experiment_id="score_rules/edge_flip"))
}, 
"edge_flip_comparison", 
"Edge flip score values for common trajectory topologies", 
15, 
5
)
```


# Acknowledgements
# References 


<div id="refs"></div>


# Supplementary Material


```{r "supplementary", echo=FALSE, results='asis'}
source("analysis/paper/sfigs_stables.R")
```


# Colophon


This report was generated on `r Sys.time()` using `r devtools:::platform_info()$version` and the following packages:
```{r colophon, cache = FALSE}
devtools::session_info()$packages %>% knitr::kable()
```


The current Git commit details are:


```{r}
git2r::repository(".")
```